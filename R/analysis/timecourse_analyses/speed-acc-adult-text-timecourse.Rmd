---
title: "Speed-acc-adult-text timecourse"
author: "Kyle MacDonald"
output: html_document
---

This script plots and analyzes the timecrouse eye-tracking data for an experiment comparing speed-accuracy tradeoffs when establishing reference in real-time. We vary the information value and saliency of the center fixation and measure the effects on accuracy and RT.

## Setup

Load libraries and read in tidy data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F, warning=F, cache=F, message=F, sanitize = T, results = 'asis')
source(here::here("R/helper_functions/libraries_and_functions.R"))
source(here::here("R/helper_functions/permutation_helpers.R"))
data_path <- "data/3_final_merged_data/timecourse/"
```

Read data. 

```{r read data}
d <- read_csv(here::here(data_path, "speed_acc_adult_text_timecourse_tidy.csv.gz"))
```

## Visualize looking behavior 

Filter to get the anlaysis window (one second before the noun onset). Start is when the center fixation appeared. End is the end of the trial. 

```{r}
d.filt <- d %>% filter(t.stim > 0, t.stim <= 5) 
```

First, we generate curves that represent center, target, and distractor looking over time

Summarize data for each ROI for each participant and each time slice.

```{r}
# get number of trials looking at each gaze target for each time slice
ss.d <- d.filt %>% 
  group_by(subid, t.stim, condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, t.stim), condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 1 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + t.stim + target_looking + condition, 
                            data = d.filt),
                      stringsAsFactors = F) %>% 
  mutate(t.stim = as.numeric(t.stim)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "t.stim", "condition")) %>% 
  mutate(proportion_looking = Freq / count)
```

Get means and CIs for proportion looking at each time slice across particpants

```{r}
ms.means.timeslice <- ss.d %>% 
  group_by(t.stim, condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))
```

Now make the Tanenhaus style curves for each condition 

```{r, fig.width=10}
ggplot(aes(x = as.numeric(t.stim), y = mean, 
           color = condition), data = ms.means.timeslice) + 
  ylim(0,1) +
  xlim(-0.5,5) +
  geom_line(size=1) +
  facet_grid(target_looking~.) +
  xlab("Time in sec from onset of trial") +
  geom_vline(xintercept = mean(d$noun_onset_sec_1), linetype = "dashed") +
  geom_vline(xintercept = mean(d$sentence_onset_sec), linetype = "dashed") +
  ylab("Proportion looking") +
  guides(color = F, shape = F, linetype = F) +
  geom_dl(aes(label = condition), method = "first.bumpup") +
  annotate("text", label = "Sentence Onset", x = mean(d$sentence_onset_sec)-0.3, 
           y = 0.4, size = 4, colour = "dodgerblue") +
  annotate("text", label = "Noun Onset", x = mean(d$noun_onset_sec_1)-0.3, 
           y = 0.4, size =4 , colour = "dodgerblue") +
  theme_minimal() + 
  theme(text = element_text(size = 12))
```

## Standard "window" accuracy analyses

### Boxplot of proportion looking to each ROI as a function of condition.

First we need to filter to get the analysis window

```{r}
d.analysis <- d.filt %>% filter(t.rel.sentence >= 0)  
```

Then we get the proportion looking to each location for each trial for each participant.

```{r}
# get number of trials looking at each gaze target for each time slice
ss.d.tr <- d.analysis %>% 
  group_by(subid, tr.num, condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 1 of this chunk
ss.d.tr <- as.data.frame(xtabs(~ subid + tr.num + target_looking + condition, 
                            data = d.analysis),
                      stringsAsFactors = F) %>% 
  mutate(tr.num = as.numeric(tr.num)) %>% 
  left_join(x = ss.d.tr, y = ., by = c("subid", "tr.num", "condition")) %>%
  mutate(proportion_looking = Freq / count) %>% 
  group_by(subid, condition, target_looking) %>% 
  summarise(m_prop_looking = mean(proportion_looking, na.rm = T))
```

Now we can plot the trial level summarized looking data. Here we use a boxplot to show both raw and summaries of the data. 

```{r, fig.width=8}
ggplot(aes(x= target_looking, y = m_prop_looking, fill = condition), 
       data = ss.d.tr) +
  geom_boxplot(alpha = 0.8) +
  geom_point(alpha = 0.6, 
             position = position_jitterdodge(jitter.width = 0.1,
                                             dodge.width = 0.75)) +
  labs(x = "Gaze Location",
       y = "Prop. Looking") +
  theme(text = element_text(size = 16))
```

Participants look less at the center fixation when it's a static bullseye, more when it's a face or text with audio, and the most when it is just text. Note that both target and distracter looking are higher for bullseye condition because they are more likely to be looking at either picture during the time before noun onset. This tells me that window accuracy is not the best metric here (at least over the entire trial window).

### Analyze looking behavior prior to central fixation onset 

```{r}
d.order.analysis <- d.filt %>% 
  filter(t.rel.center.fixation <= 0) %>%
  group_by(subid, tr.num, condition, audio, target_looking) %>%
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, tr.num, condition, audio), target_looking, 
           fill = list(count=0)) %>% 
  group_by(subid, tr.num) %>%
  mutate(prop_looking = count / sum(count))
```

```{r}
d.order.analysis %>% 
  filter(target_looking == "center") %>% 
  ggplot(aes(x = prop_looking, fill = condition)) + 
  geom_density(alpha = 0.5) 
```

### Does looking to center prior to fixation onset change over time in the task?

```{r}
d.order.analysis %>% 
  ungroup() %>% 
  filter(target_looking == "center") %>% 
  mutate(tr.num = as.numeric(tr.num), 
         tr.binned = cut(tr.num, breaks = 6)) %>% 
  group_by(tr.binned) %>% 
  summarise(m = mean(prop_looking),
            ci = (qnorm(0.975)*sd(prop_looking)) / sqrt(length(prop_looking))) %>% 
  ggplot(aes(x = tr.binned, y = m)) + 
  geom_pointrange(aes(ymin = m - ci, ymax = m + ci)) +
  geom_line(group = 1, size = 1) +
  ylim(0.4, 1) +
  geom_hline(yintercept = 0.5, linetype = "dashed")  +
  labs(x = "Binned Trial Number", 
       y = "Prop. Looks to Center",
       title = "Anticipatory looking to center") +
  theme(text = element_text(size = 16))
```

Looks like there is a an increase in looking away from the two target images to the center area as the task unfolds. I think this is an anticipation effect -- Participants know that the information source is going to appear in the same spot, so they fixate there.

But this analysis ignores the different conditions. This is what we look at next:

```{r}
create_tr_num_fun <- function(data_frame) {
  
  rows_df <- data_frame %>% 
    select(subid, condition, tr.num) %>% 
    unique() %>% 
    nrow()
  
  data_frame %<>% 
    select(subid, condition, tr.num) %>% 
    unique() %>% 
    mutate(tr_cond = seq(1, rows_df),
           block = ifelse(tr.num <= 24, 1, 
                          ifelse(tr.num > 24 & tr.num <= 48, 2, 
                                 ifelse(tr.num > 48 & tr.num <= 72, 3, 4)))) %>% 
    left_join(data_frame, ., by = c("subid", "tr.num", "condition"))
  
  data_frame
}
```

Add the trial number within condition to our data frame.

```{r}
d.order.analysis %<>% 
  group_by(condition, subid) %>% 
  do(create_tr_num_fun(data_frame = .))
```

Now summarise and plot.

```{r}
d.order.analysis %>% 
  ungroup() %>% 
  filter(target_looking == "center") %>% 
    mutate(tr_cond = as.numeric(tr_cond), 
         tr.binned = cut(tr_cond, breaks = 4)) %>% 
  group_by(tr.binned, condition) %>% 
  summarise(m = mean(prop_looking),
            ci = (qnorm(0.975)*sd(prop_looking)) / sqrt(length(prop_looking))) %>% 
  ggplot(aes(x = tr.binned, y = m, color = condition, group = condition)) + 
  geom_pointrange(aes(ymin = m - ci, ymax = m + ci), 
                  position = position_jitter(width = 0.05)) +
  geom_line(size = 1) +
  ylim(0.4, 1) +
  scale_x_discrete(expand = c(0,1.2)) +
  guides(color=F) +
  geom_dl(aes(label = condition), method = list("first.bumpup", hjust = 1.1, cex = 1.2)) +
  geom_hline(yintercept = 0.5, linetype = "dashed")  +
  labs(x = "Binned Trial Number", 
       y = "Prop. Looks to Center",
       title = "Change in anticipatory looking to center") +
  theme(text = element_text(size = 12))
```

## Cluster-based permutation test of center looking curves

Filter timecourse to analysis window.

```{r}
conds_to_compare <- c("text-no-audio", "text")
d_cluster <- d.filt %>% filter(t.rel.noun >= 0, t.rel.noun <= 2, condition %in% conds_to_compare)  

# Create time bins for each trial and flag bins with participant data in both conditions.
d_cluster %<>% 
  group_by(subid, tr.num) %>% 
  arrange(t.stim) %>% 
  mutate(Bin = 1:n())

# use this null df to test the cb test pipeline
d_cluster_null <- d_cluster %>% mutate(condition = "text-null") %>% bind_rows(d_cluster)
```

Aggregat prop looking data.

```{r}
# Aggregate at the subject level
ss.d <- d_cluster %>% 
  group_by(subid, Bin, condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, Bin), condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 2 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + Bin + target_looking + condition, 
                            data = d_cluster),
                      stringsAsFactors = F) %>% 
  mutate(Bin = as.numeric(Bin)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "Bin", "condition")) %>% 
  mutate(proportion_looking = Freq / count)
```

Plot things just to make sure the transformations didn't do anything crazy.

```{r}
ms.means.timeslice <- ss.d %>% 
  group_by(Bin, condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))

ms.means.timeslice %>% 
  filter(Bin <= 60) %>% 
  ggplot(aes(x = as.numeric(Bin), y = mean, 
           color = target_looking, linetype = condition), data = .) + 
  lims(y = c(0,1), 
       x = c(0, 65)) +
  geom_line(size=1, position = position_dodge(width = 0.1)) +
  guides(color = F) +
  xlab("Time in sec from onset of trial") +
  ylab("Proportion looking") +
  geom_dl(aes(label = target_looking), method = "last.bumpup") +
  labs(linetype = "noise condition: ") +
  theme(legend.position = "top") +
  scale_color_manual(values = c("dodgerblue", "red", "forestgreen"))
```

Rename variables to work with cluster-based permutation functions. We also create a varible to track complete cases for the paired t-test.

```{r}
ss_filt <- ss.d %>% 
  filter(!is.na(proportion_looking),
         target_looking == "center") %>% 
  mutate(trial_type = condition,
         ss_m = proportion_looking,
         Participant = subid) %>% 
  group_by(subid, Bin) %>% 
  mutate(n = n(),
         complete_case = ifelse(n == 2, "complete", "incomplete"),
         ss_m = ifelse(condition == "text-null", ss_m + rnorm(n=1), ss_m))

ss_complete <- ss_filt %>% filter(complete_case == "complete")
```

### Get clusters 

```{r}
# Define thresholds for t-test
num_sub = length(unique((d$subid)))
threshold_t = qt(p = 1 - .05 / 2, df = num_sub - 1) # pick threshold t based on alpha = .05 two tailed

# Nest the data by time bin 
by_bin <- ss_complete %>% 
  group_by(Bin) %>% 
  nest()

# Find and filter by the longest timepoint to make sure we have 
# the same number of data points to comapre across conditions
bin_cut_point <- ss_complete %>% 
  group_by(trial_type) %>% 
  summarise(max_bin = max(Bin)) %>% 
  pull(max_bin) %>% 
  min()

by_bin %<>% filter(Bin <= bin_cut_point)
```

```{r}
# 1. Map the t-test to each time bin and store it as a variable in the by_bin data frame. 
by_bin %<>% mutate(model = map(data, run_t_test, paired = TRUE, alternative = "two.sided"))

# 2. Take the time-bins whose test passed the threshold statistic 
# (e.g., t > 2), and group them by adjacency. We will call these 
# time-clusters. To do this, we extract the t-stats and their 
# bin numbers and filter by the threshold defined above.

# get t-stats for each model
t_df <- by_bin %>% 
  mutate(glance = map(model, broom::glance)) %>% 
  unnest(glance, .drop = TRUE) %>% 
  select(Bin, statistic, p.value)

# filter by t threshold 
t_df_filt <- t_df %>% filter(abs(statistic) >= threshold_t)

# 3. For each time-cluster, calculate the sum of the 
# statistics for the time-bins inside it.
t_df_filt %<>% define_clusters()

# 4. Next, we calculate the sum of the statistics for the time-bins inside each cluster.
observed_t_vals <- t_df_filt %>% 
  group_by(cluster) %>% 
  summarise(sum_t = sum(statistic)) 

observed_t_vals %>% head()
```

Take the largest of the cluster-level statistics and store it for later. This is the t-value that we will use to compare against our null distribution generated by the permutations that we will create in the next part of the program.

```{r}
largest_t <- observed_t_vals %>% pull(sum_t) %>% abs() %>% max()
```

### Generate null distribution by shuffling data

We will take our data and randomly shuffle it, assigning participants randomly to each condition. Then we will perform steps (1)-(3) on the shuffled dataset and save the biggest sum-statistic from each permutation analysis. 

So we create a list of data frames. Each data frame is a permuted dataset.

```{r, message=F}
# global variables for setting up shuffling and 
set.seed(7)
n_permutations <- 1000
permuted_dfs <-  vector("list", length(1:n_permutations))

for (n_sim in 1:n_permutations) {
  permuted_dfs[[n_sim]] <- permute_data(ss_complete, within = T)
}

# Now that we have our list of permuted datasets, we map the statistical test 
# using a wrapper function to each time bin in each dataset. *
# *Note that this takes about 4.5 minutes to do 1000 permutations.**
# currently does not support paired T-tests

system.time(
  null_dist_1 <- map(permuted_dfs, t_test_by_bin, paired = T, alternative = "two.sided", bin_cut = bin_cut_point)
)

# Next, we need to extract the t-stats, filtering by the pre-defined threshold. 
# **Note that this takes about 4.5 minutes to do 1000 permutations.**
system.time(
 null_dist_2 <- map(null_dist_1, extract_t, t_threshold = threshold_t, alternative = "two.sided") 
)

# Then, we make clusters for each filtered data frame of Bins that have above-threshold t-values.
null_dist_3 <- map(null_dist_2, define_clusters)  

# Then, we sum the t-stats for each cluster and store the largest value for each simulation. 
null_dist_4 <- map(null_dist_3, sum_t_stats) %>% unlist() %>% data.frame(null_t = .)  

beepr::beep(sound = 3)
```

Now we have a data frame with a single column that is a distribution of t-stats from the randomly shuffled datasets. We can use this as a null distribution against which we can compare the cluster-statistic measured from our actual data in step (3). This will get us a p-value, or the probability of that we would have observed our data or something more extreme if the null were true.

We can visualize this null distribution along with our sample sum-statistic.

```{r}
null_dist_4 %>% 
  ggplot(aes(x = null_t)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = sum_t),
             data = observed_t_vals,
             color = "darkorange", 
             linetype = "dashed",
             size = 1) +
  theme_classic()
```

Finally, to compute the p-value we compare the cluster-statistic from step (3) to the distribution found in (6). 

```{r}
t_value_to_test <- largest_t

nulls <- null_dist_4 %>% 
  filter(!is.na(null_t)) %>% 
  mutate(sig = abs(null_t) >= abs(t_value_to_test)) %>% # test if the null-sum stat is greater than the the sum-stat for the largest cluster
  pull(sig)

crit_p <- ( sum(nulls) / length(nulls) ) %>% round(3) # get the p-value rounded to 3 digits
crit_p
```

All ps for text-no-audio center looking curves < .001
