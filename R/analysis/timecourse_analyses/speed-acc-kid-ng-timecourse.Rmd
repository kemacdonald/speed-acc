---
title: "Timecourse plots Noise-Gaze Kids"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

This script plots and analyzes the timecrouse eye-tracking data for an experiment comparing speed-accuracy tradeoffs when establishing reference in real-time. We vary the information value and saliency of the center fixation and measure the effects on accuracy and RT.

## Setup

Load libraries and read in tidy data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F, warning=F, cache=F, message=F, sanitize = T, results = 'asis')
source(here::here("R/helper_functions/libraries_and_functions.R"))
source(here::here("R/helper_functions/permutation_helpers.R"))
data_path <- "data/3_final_merged_data/timecourse/"
```

Read data. 

```{r read data}
d <- read_csv(here::here(data_path, "speed_acc_child_gaze_timecourse_tidy.csv.gz"))
d_noise <- read_csv(here::here(data_path, "speed_acc_child_noise_timecourse_tidy.csv.gz")) 
```

## Visualize looking behavior 

Filter to get the analysis window (one second before the noun onset). Start is when the center fixation appeared. End is the end of the trial. 

```{r}
d.filt <- d %>% filter(t.stim > 0, t.stim <= 9) 
```

First, we generate curves that represent center, target, and distractor looking over time

Summarize data for each ROI for each participant and each time slice.

```{r}
# get number of trials looking at each gaze target for each time slice
ss.d <- d.filt %>% 
  group_by(subid, t.stim, gaze_condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, t.stim), gaze_condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 1 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + t.stim + target_looking + gaze_condition, 
                            data = d.filt),
                      stringsAsFactors = F) %>% 
  mutate(t.stim = as.numeric(t.stim)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "t.stim", "gaze_condition")) %>% 
  mutate(proportion_looking = Freq / count)
```

Get means and CIs for proportion looking at each time slice across particpants

```{r}
ms.means.timeslice <- ss.d %>% 
  group_by(t.stim, gaze_condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))
```

Now make the Tanenhaus style curves for each gaze_condition 

```{r}
plot <- ms.means.timeslice %>% 
  ggplot(aes(x = as.numeric(t.stim), y = mean, 
           color = target_looking, linetype = gaze_condition), data = .) + 
  ylim(0,1) +
  xlim(0,10.5) +
  geom_line(size=1) +  
  #facet_wrap(~noise_gaze_condition, ncol = 1) +
  guides(color = F) +
  xlab("Time in sec from onset of trial") +
  geom_vline(xintercept = mean(d$noun_onset_seconds), linetype = "dashed") +
  geom_vline(xintercept = mean(d$sentence_onset_seconds), linetype = "dashed") +
  ylab("Proportion looking") +
  geom_dl(aes(label = target_looking), method = "last.bumpup") +
  #annotate("text", label = "Sentence Onset", x = mean(d$sentence_onset_seconds)-0.3, 
           #y = 0.4, size = 4, colour = "dodgerblue") +
  #annotate("text", label = "Noun Onset", x = mean(d$noun_onset_seconds)-0.3, 
   #        y = 0.4, size = 4 , colour = "dodgerblue") +
  theme_bw() +
  theme(text = element_text(size = 18),
        legend.position = "top") 
```

## Noise timecourse

```{r}
d_filt <- d_noise %>% filter(t.stim > 0, t.stim <= 9) 
```

Aggregate looking behavior

```{r}
# get number of trials looking at each gaze target for each time slice
ss.d <- d_filt %>% 
  group_by(subid, t.stim, noise_condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, t.stim), noise_condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 1 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + t.stim + target_looking + noise_condition, 
                            data = d_filt),
                      stringsAsFactors = F) %>% 
  mutate(t.stim = as.numeric(t.stim)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "t.stim", "noise_condition")) %>% 
  mutate(proportion_looking = Freq / count)
```

Get means and CIs for proportion looking at each time slice across particpants

```{r}
ms.means.timeslice <- ss.d %>% 
  group_by(t.stim, noise_condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))
```

Now make the Tanenhaus style curves for each noise_condition 

```{r}
ms.means.timeslice %<>%
  ungroup() %>% 
  mutate(noise_condition = ifelse(noise_condition == "no_noise", 
                                  "clear", noise_condition))

ms.means.timeslice %>% 
  ggplot(aes(x = as.numeric(t.stim), y = mean, 
           color = target_looking, linetype = noise_condition), data = .) + 
  ylim(0,1) +
  xlim(0,10.5) +
  geom_line(size=1) +
  #facet_wrap(~noise_noise_condition, ncol = 1) +
  guides(color = F) +
  xlab("Time in sec from onset of trial") +
  geom_vline(xintercept = min(d_noise$noun_onset_seconds), linetype = "dashed") +
  geom_vline(xintercept = min(d_noise$sentence_onset_seconds), linetype = "dashed") +
  ylab("Proportion looking") +
  geom_dl(aes(label = target_looking), method = "last.bumpup") +
  annotate("text", label = "Sentence Onset", x = min(d_noise$sentence_onset_seconds)-1.2, 
           y = 1, size = 4, colour = "black") +
  annotate("text", label = "Noun Onset", x = min(d_noise$noun_onset_seconds)+1, 
           y = 1, size = 4 , colour = "black") +
  theme_bw() +
  labs(linetype = "noise condition: ") +
  theme(legend.position = "top") +
  scale_color_manual(values = c("dodgerblue", "red", "forestgreen"))
```

## Cluster-based permutation analysis

Filter timecourse to analysis window.

```{r}
d_cluster <- d_filt %>% filter(t.rel.noun >= 0, t.rel.noun <= 2)  
```

Create time bins for each trial and flag bins with participant data in both conditions.

```{r}
d_cluster %<>% 
  group_by(subid, tr.num) %>% 
  arrange(t.stim) %>% 
  mutate(Bin = 1:n())
```

Aggregate at the subject level

```{r}
# get number of trials looking at each gaze target for each time slice
ss.d <- d_cluster %>% 
  group_by(subid, Bin, noise_condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, Bin), noise_condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 1 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + Bin + target_looking + noise_condition, 
                            data = d_cluster),
                      stringsAsFactors = F) %>% 
  mutate(Bin = as.numeric(Bin)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "Bin", "noise_condition")) %>% 
  mutate(proportion_looking = Freq / count)
```

Plot things just to make sure the transformations didn't do anything crazy.

```{r}
ms.means.timeslice <- ss.d %>% 
  group_by(Bin, noise_condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))

ms.means.timeslice %>% 
  filter(Bin <= 60) %>% 
  ggplot(aes(x = as.numeric(Bin), y = mean, 
           color = target_looking, linetype = noise_condition), data = .) + 
  lims(y = c(0,1), 
       x = c(0, 65)) +
  geom_line(size=1) +
  guides(color = F) +
  xlab("Time in sec from onset of trial") +
  ylab("Proportion looking") +
  geom_dl(aes(label = target_looking), method = "last.bumpup") +
  theme_bw() +
  labs(linetype = "noise condition: ") +
  theme(legend.position = "top") +
  scale_color_manual(values = c("dodgerblue", "red", "forestgreen"))
```

Rename variables to work with cluster-based permutation functions. We also create a varible to yrack complete cases for the paired t-test.

```{r}
ss_filt <- ss.d %>% 
  filter(!is.na(proportion_looking),
         target_looking == "center") %>% 
  mutate(trial_type = noise_condition,
         ss_m = proportion_looking,
         Participant = subid) %>% 
  group_by(subid, Bin) %>% 
  mutate(n = n(),
         complete_case = ifelse(n == 2, "complete", "incomplete"))
```

Filter to only keep complete cases.

```{r}
ss_complete <- ss_filt %>% filter(complete_case == "complete")
```

Define thresholds for t-test

```{r}
num_sub = length(unique((d_noise$subid_short)))
threshold_t = qt(p = 1 - .05 / 2, df = num_sub-1) # pick threshold t based on alpha = .05 two tailed
```

Nest the data by time bin 

```{r}
by_bin <- ss_complete %>% 
  group_by(Bin) %>% 
  nest()
```

Find and filter by the longest timepoint to make sure we have the same number of data points to comapre across conditions

```{r}
bin_cut_point <- ss_complete %>% 
  group_by(trial_type) %>% 
  summarise(max_bin = max(Bin)) %>% 
  pull(max_bin) %>% 
  min()

by_bin %<>% filter(Bin <= bin_cut_point)
```

1. Map the t-test to each time bin and store it as a variable in the by_bin data frame. 

```{r}
by_bin %<>% mutate(model = map(data, run_t_test, paired = TRUE, alternative = "two.sided"))
```

2. Take the time-bins whose test passed the threshold statistic (e.g., t > 2), and group them by adjacency. We will call these time-clusters. To do this, we extract the t-stats and their bin numbers and filter by the threshold defined above.

```{r}
# get t-stats for each model
t_df <- by_bin %>% 
  mutate(glance = map(model, broom::glance)) %>% 
  unnest(glance, .drop = TRUE) %>% 
  select(Bin, statistic, p.value)

# filter by t threshold 
t_df_filt <- t_df %>% filter(abs(statistic) >= threshold_t)
t_df_filt
```

3. For each time-cluster, calculate the sum of the statistics for the time-bins inside it.

The technical challenge here is to figure out how to define the clusters based on the Bin vector. 

```{r}
t_df_filt %<>% define_clusters()
t_df_filt
```
4. Next, we calculate the sum of the statistics for the time-bins inside each cluster.

```{r}
observed_t_vals <- t_df_filt %>% 
  group_by(cluster) %>% 
  summarise(sum_t = sum(statistic)) 

observed_t_vals
```

Take the largest of the cluster-level statistics and store it for later. This is the t-value that we will use to compare against our null distribution generated by the permutations that we will create in the next part of the program.

```{r}
largest_t <- observed_t_vals %>% pull(sum_t) %>% abs() %>% max()
largest_t
```

### Generate null distribution by shuffling data

We will take our data and randomly shuffle it, assigning participants randomly to each condition. Then we will perform steps (1)-(3) on the shuffled dataset and save the biggest sum-statistic from each permutation analysis. 

So we create a list of data frames. Each data frame is a permuted dataset.

```{r, message=F}
# global variables for setting up shuffling and 
set.seed(7)
n_permutations <- 1000
permuted_dfs <- vector("list", length(1:n_permutations))

# change condition column name so function will work
ss_complete %<>% rename(condition = noise_condition)

system.time(
  for (n_sim in 1:n_permutations) {
    permuted_dfs[[n_sim]] <- permute_data(ss_complete)
  }
)
```

Now that we have our list of permuted datasets, we map the statistical test using a wrapper function to each time bin in each dataset. **Note that this takes about 4.5 minutes to do 1000 permutations.**

```{r}
system.time(
  null_dist_1 <- map(permuted_dfs, t_test_by_bin, paired = FALSE, 
                     alternative = "two.sided", bin_cut = bin_cut_point)
)
```

Next, we need to extract the t-stats, filtering by the pre-defined threshold. **Note that this takes about 4.5 minutes to do 1000 permutations.**

```{r}
system.time(
 null_dist_2 <- map(null_dist_1, extract_t, t_threshold = threshold_t, alternative = "two.sided") 
)
```

Then, we make clusters for each filtered data frame of Bins that have above-threshold t-values.

```{r}
system.time(
  null_dist_3 <- map(null_dist_2, define_clusters)  
)
```

Then, we sum the t-stats for each cluster and store the largest value for each simulation. 

```{r}
system.time(
  null_dist_4 <- map(null_dist_3, sum_t_stats) %>% unlist() %>% data.frame(null_t = .)  
)


```

Now we have a data frame with a single column that is a distribution of t-stats from the randomly shuffled datasets. We can use this as a null distribution against which we can compare the cluster-statistic measured from our actual data in step (3). This will get us a p-value, or the probability of that we would have observed our data or something more extreme if the null were true.

We can visualize this null distribution along with our sample sum-statistic.

```{r}
null_dist_4 %>% 
  ggplot(aes(x = null_t)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = sum_t),
             data = observed_t_vals,
             color = "darkorange", 
             linetype = "dashed",
             size = 1) +
  theme_classic()
```

Finally, to compute the p-value we compare the cluster-statistic from step (3) to the distribution found in (6). 

```{r}
t_value_to_test <- largest_t

nulls <- null_dist_4 %>% 
  filter(!is.na(null_t)) %>% 
  mutate(sig = abs(null_t) >= abs(t_value_to_test)) %>% # test if the null-sum stat is greater than the the sum-stat for the largest cluster
  pull(sig)

crit_p <- ( sum(nulls) / length(nulls) ) %>% round(3) # get the p-value rounded to 3 digits
crit_p
```