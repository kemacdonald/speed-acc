---
title: "Speed-Acc Child Noise Wrangle"
author: "Kyle MacDonald"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=T, warning=F, cache=F, message=F, sanitize = T)
source("../../helper_functions/libraries_and_functions.R")
```

This script munges eye-tracking data for an experiment comparing speed-accuracy tradeoffs when establishing reference in real-time. We vary the information value and saliency of the center fixation and measure the effects on accuracy and RT.

Load data and join with trial information (timing, left/right/center images, target info) and with participant info.

```{r load data}
#### read in processed data
d <- read_csv("../../../data/2_cleaned_data/speed_acc_processed_child_noise.csv.gz")

#### strip .avi and .jpg from stimulus
d$stimulus <- gsub(pattern = ".jpg", "", x = d$stimulus)
d$stimulus <- gsub(pattern = ".avi", "", x = d$stimulus)

### remove validation trial
d %<>% filter(stimulus != "Validation")
```

Join trial info

```{r}
trialinfo_d <- read_csv("../../../data/0b_trial_information/speed-acc-child-noise-trial-info.csv")
d %<>% left_join(., trialinfo_d, by = "stimulus")
```

Join subject info

```{r}
subinfo_df <- readxl::read_xlsx("../../../data/0a_demographics/speed_acc_child_noise_Runsheet_Bing.xlsx")

# clean up subid to join
subinfo_df %<>% mutate(subid = str_to_lower(subid),
                       subid = str_replace_all(subid, pattern = "-", "_"))
# clean up subid in data file
# if "child" is already in the subid, then we do not need to add it
d %<>% mutate(subid = str_to_lower(subid),
                       subid = str_replace_all(subid, pattern = "-", "_"),
                       subid = ifelse(str_detect(subid, pattern = "child"), 
                                      subid,
                                      str_replace(subid, pattern = "_noise",
                                                             "_child_noise")
                                      ))

d %<>% left_join(subinfo_df, by = "subid")
```

Filter out calibration trial and any keypress events, and outro.

```{r}
bad_stimuli <- c("UE-keypress VolumeUp", "UE-keypress VolumeDown",
                 "cc3bd7c5c8d457694031c7630357751f", 
                 "85e91b277ab3c43334caff8617a99a27_1920x1080",
                 "a14e8d2f42de904c74b227f460058f80_1920x1080",
                 "c7280d7cf8d2071e0a099f77fef07087",
                 "cb574a221f2b82f8cd9ab49f3da75b49_1920x1080",
                 "ee996bde25388f5c35c6bd48740a00d6_1920x1080",
                 "2cfc78e6145270a07971626e1be0caf5_1920x1080",
                 "6cc2283d5439516a7f183c460278aed5_1920x1080",
                 "074d7edcd6e1aaa51346a9225dc5e669_1920x1080")

d %<>% filter(!(stimulus %in% bad_stimuli))
```

### Filter out unusable participants 

We use our lab criterion of having a proprtion of AWAY looks +3 standard deviations away from the mean.

```{r filter out participants with bad data}
##calculate percentage of non-looks - that is, looking away from the screen entirely.
##offscreen == TRUE if x is 0 and y is 1050, so higher bad_looks are worse
d_screentime <- transform(d, offscreen = ((x == 0 & y == 1050) | is.na(x) | is.na(y))) %>%
  select(subid, t, offscreen) %>%
  group_by(subid) %>%
  summarise(bad_looks = sum(offscreen, na.rm = TRUE), total_timeslices = n())

d_screentime <- transform(d_screentime, percent_bad_looks = bad_looks/total_timeslices)

##filter out participants more than 3 standard dev away from mean screentime
sd_ss <- sd(d_screentime$percent_bad_looks)
mean_ss <- mean(d_screentime$percent_bad_looks)

d %<>% left_join(., d_screentime, by = "subid")

d$keep <- ifelse(d$percent_bad_looks > (3*sd_ss + mean_ss), 'exclude', 'include')
```

Now we make a table of participant information.

```{r}
d %>% select(subid, gender, run_date, comments) %>% 
  unique() %>% 
  kable()
```

## Data cleaning/munging

Here we label Areas of Interest to be Left Picture, Right Picture, and the Center Fixation. We also add trial number variable.

### ROIs

Plot the distribution of looking across x-y coordinates. This is a useful check that participants were looking where we thought they should be, and it helps us find the AOIs for data analysis. 

```{r, eval = F}
d.rois <- sample_n(d, size = 0.1*nrow(d)) # get a sample so we don't plot all 200k plus rows

ggplot(aes(x = x, y = y), data = d.rois) +
  geom_density2d() +
  theme_bw()
```

### Remove filler trials

```{r}
d %<>% filter(!(is.na(target_image)))
```


### Add trial numbers

We have 2 blocks of 16 trials, so `r 2 * 16` total trials. To add the trial numbers we first get each unique trial name and the time it started. Then we sort by the time each trial started, and bind the trial numbers. Finally, we join the trial number information with the rest of the eye movement data.

```{r add trial numbers}
# apply the add trials function to each subid's data frame using the do() operator 
d %<>% 
  group_by(subid) %>% 
  do(add.tr.nums.fun(.)) %>% 
  left_join(x = d, y = ., by = c("subid", "stimulus"))
```

Check that we have the right number of trials for each participant: 32 trials

```{r}
d %>% 
  select(subid, tr.num) %>% 
  unique() %>% 
  group_by(subid) %>% 
  count() %>% 
  kable()
```

Extract short subid

```{r}
d %<>% mutate(subid_short = as.numeric(str_extract(subid, "[:digit:]+")),
              experiment = "noise")
```

### Flag left/right/center looking based on ROIs

The bounds of the ROIs are generated by visual inspection of the distribution of looks in the task

```{r flag l/r/c looking}
d %<>%
  mutate(gaze_target = ifelse(x >= 0 & x <= 960  & y >= 0 & y <= 500, "left",
                              ifelse(x > 960 & x <= 1920 & y >= 0 & y <= 500, "right",
                                     ifelse(x >= 600 & x <= 1200 & y > 500 & y <= 1080, "center",
                                            "away"))))
```

Make a table of proportion looking to each area of interest: Left, Right, Center, and Away

```{r}
d %>% 
  filter(is.na(gaze_target) == F) %>% 
  group_by(gaze_target) %>% 
  summarise(count_looks = n()) %>% 
  mutate(total_looks = sum(count_looks),
         prop_looks = round(count_looks / total_looks, 2)) %>% 
  kable()
```

### Score each trial: RT and Accuracy of first shift

Filter out "away" looks and create some useful variables: condition, target side of the screen, and whether the participant was looking at the target vs. distracter at each time point.

And for our RT analyses, we need to mark the critical onset for each trial. Here we mark three time points of interest. Time relative to the:

1. center fixation appearing on the screen
2. sentence onset
3. target noun onset

First, we need to convert the trial measurement information from frames to milliseconds and seconds.

```{r}
d %<>% 
  mutate(noun_onset_ms = (noun_onset_sec * 1000) + (noun_onset_frames * 33),
         noun_onset_seconds = noun_onset_ms / 1000,
         sentence_onset_ms = (sentence_onset_sec * 1000) + (sentence_onset_frames * 33),
         sentence_onset_seconds = sentence_onset_ms / 1000)
```

```{r}
center.fix.onset <- 2.5 # experiment was programmed such that center fixation appeared 2.5 sec after trial onset

d %<>% filter(gaze_target != "away", is.na(gaze_target) == F) %>% 
  mutate(t.stim = ifelse(t.stim == 0, 0, round(t.stim, digits = 3)),
         target_side = ifelse(target_image == left_image, "left", "right"),
         target_looking = ifelse(gaze_target == "center", "center",
                                 ifelse(gaze_target == target_side, "target", 
                                        "distracter")),
         t.rel.noun = t.stim - noun_onset_seconds,
         t.rel.center.fixation = t.stim - center.fix.onset,
         t.rel.sentence = t.stim - sentence_onset_seconds)
```

### Flag where participant was looking at each of the critical points in the trial 

In the next set of chunks, we will condense the timecourse information into fewer bits. Specifically, we want to get the information about each participants' first shift on every trial using three different critical onset points. The three onsets of interest are: 

* center fixation (when the center target appeared on the screen)
* sentence onset (when the carrier phrase "Look! Where's the ________" began)
* noun onset (the start of the target noun in the sentence)

```{r flag gaze target at crit onsets for each trial}
# create a temporary copy of the data frame to join at each step
d.orig <- d

# this is how many rows we expect to have after creating the three new response variables
n_row_final <- nrow(d.orig) * 3

# noun onset
d <- d.orig %>% 
  filter(t.rel.noun > 0) %>%
  group_by(tr.num, subid) %>% 
  do(filter(., t.rel.noun == min(t.rel.noun))) %>% 
  mutate(response = target_looking,
         response_onset_type = "noun") %>% 
  select(subid, tr.num, response, response_onset_type) %>% 
  left_join(d.orig, ., by = c("subid", "tr.num"))

# center fixation
d <- d.orig %>% 
  filter(t.rel.center.fixation > 0) %>%
  group_by(tr.num, subid) %>% 
  do(filter(., t.rel.center.fixation == min(t.rel.center.fixation))) %>% 
  mutate(response = target_looking,
         response_onset_type = "center.fixation") %>%
  select(subid, tr.num, response, response_onset_type) %>% 
  left_join(d.orig, ., by = c("subid", "tr.num")) %>% 
  bind_rows(d, .)

# sentence onset
d <- d.orig %>% 
  filter(t.rel.sentence > 0) %>%
  group_by(tr.num, subid) %>% 
  do(filter(., t.rel.sentence == min(t.rel.sentence))) %>% 
  mutate(response = target_looking,
         response_onset_type = "sentence") %>% 
  select(subid, tr.num, response, response_onset_type) %>% 
  left_join(d.orig, ., by = c("subid", "tr.num")) %>% 
  bind_rows(d, .)
```

Next, we need to compute RT for each trial, using my own score trial function. 

The function takes in a data frame for each trial and computes an RT and whether the shift was correct or incorrect. You can tell the function the kind of onset value from which you want to compute RT. The different onset values are:

* noun 
* center_fixation
* sentence

Now we apply the score trial function to each trial for each participant in the tidy data frame using the do() functionality in dplyr. Note that we are using the .$ index to use a variable from the piped data frame as an argument to the score trial function.

```{r}
d %<>% 
  group_by(subid, tr.num, response_onset_type) %>% 
  filter(is.na(response_onset_type) == F) %>% 
  do(score_trial_et(., crit_onset_type = .$response_onset_type))
```

Check the distribution of shift types over the different response types.

```{r check freq of each shift}
d %>% 
  select(subid, tr.num, response_onset_type, shift_type) %>% 
  unique() %>% 
  group_by(response_onset_type, shift_type) %>% 
  count() %>% 
  kable()
```

Now we have an RT, a shift type, and a correct/incorrect value for each trial based on three different critical onset points in the trial (i.e., center fixation, sentence, and noun onset).

### Write tidy data to .csv file

Before writing to .csv, we split the data into the timecourse information and first shift information. This will make analysis more efficient since we won't have to load large datasets into working memory.

```{r}
# timecourse
d.timecourse <- filter(d, response_onset_type == "noun") %>% 
  select(subid, t.stim, gaze_condition, noise_condition, tr.num, noun_onset_sec,
         gaze_target:shift_accuracy, target_image, keep)

# first shift
d.fs <- d %>% 
  filter(response_onset_type == "noun") %>% # km remember to remove this line after HUMBIO
  select(subid, tr.num, gaze_condition, noise_condition, target_image, run_date:comments, 
         response:shift_accuracy, keep) %>% 
  unique()
```

Write data to zipped csv: 

```{r}
write.path.time <- "../../../data/3_final_merged_data/timecourse/"
write_csv(d.timecourse, path = paste0(write.path.time, "speed_acc_child_noise_timecourse_tidy.csv.gz"))

write.path.fst <- "../../../data/3_final_merged_data/first_shifts/"
write_csv(d.fs, path = paste0(write.path.fst, "speed_acc_child_noise_fstshift_tidy.csv"))
```