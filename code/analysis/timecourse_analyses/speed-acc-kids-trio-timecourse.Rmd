---
title: "Speed-Accuracy Trio Timecourse Plots"
author: "Kyle MacDonald"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F, warning=F, cache=F, message=F, sanitize = T)
source(here::here("R/helper_functions/libraries_and_functions.R"))
source(here::here("R/helper_functions/permutation_helpers.R"))
data_path <- "data/3_final_merged_data/timecourse/"
```

## Load data

```{r load and clean up data}
d <- read_csv(here::here(data_path, "speed-acc-child-timecourse.csv"))

d %<>% mutate(stimuli = ifelse(stimuli %in% c("V1", "V2"), "ASL", 
                               ifelse(stimuli == "Trio", "Object", 
                                      ifelse(stimuli == "Bull", "Bullseye",stimuli))),
              stimuli = factor(stimuli, levels = c("ASL", "Face", "Object", "Bullseye")))

colnames(d) <- gsub(pattern = "X", replacement = "", x = colnames(d))
```

## Tanenhaus plot for English vs. ASL

First, we munge the raw iChart data.

Grab just the eye movement data and group information

```{r}
ss_iChart <- d %>% 
  select(Sub.Num, language_modality, Tr.Num, trial_type, stimuli, age_code, Response, `0`:`3000`) %>% 
  filter(Response == "D", age_code == "child") 
```

Convert to long format

```{r}
ss_iChart_long <- ss_iChart %>% 
  gather(key = Time.ms, value = value, `0`:`3000`) %>% 
  filter(value %in% c("0", "0.5", "1")) %>% 
  mutate(value_cat = factor(value, labels = c("Distractor", "Center", "Target")),
         Time.ms_numeric = as.numeric(Time.ms)) 
```    

Summarize for each participant - get proportion looking at each time slice

```{r}
ms_iChart_count <- ss_iChart_long %>% 
  group_by(Sub.Num, Time.ms, language_modality, value_cat) %>% 
  dplyr::summarise(count = ifelse(n() == 0, 0, n())) %>% 
  dplyr::summarise(sum_count = sum(count)) %>% 
  ungroup() %>% 
  mutate(Sub.Num = as.character(Sub.Num))

ms_iChart <- as.data.frame(xtabs(~ Sub.Num + value_cat + Time.ms, data = ss_iChart_long),
                           stringsAsFactors = F) %>% 
  left_join(y = ms_iChart_count, by = c("Sub.Num", "Time.ms")) %>% 
  mutate(proportion_looking = Freq / sum_count,
         language_modality_factor = as.factor(language_modality))

# Get means and CIs for proportion looking at each time slice across particpants
ms_mean_iChart <- ms_iChart %>% 
  group_by(Time.ms, language_modality, value_cat) %>% 
  langcog::multi_boot_standard(col = "proportion_looking", na.rm = T)

breaks <- seq(0,3000, by=500)
y_breaks <- seq(0.25,1.0, by = 0.25)
points <- seq(0,3000, by = 200)

ggplot(aes(x = as.numeric(Time.ms), y = mean, 
           color = value_cat, shape = language_modality, 
           linetype = language_modality), 
       data = ms_mean_iChart) + 
  ylim(0,1) +
  geom_line(data = filter(ms_mean_iChart, as.numeric(Time.ms) %in% points), size=0.7) +
  geom_linerange(data = filter(ms_mean_iChart, Time.ms %in% points),
                 aes(ymin=ci_lower, ymax=ci_upper), alpha = 0.5) +
  geom_point(data = filter(ms_mean_iChart, as.numeric(Time.ms) %in% points), size=3) +
  scale_color_brewer(palette = "Set1", type = "qual") +
  xlab("Time in ms from onset of noun") +
  ylab("Proportion looking") +
  guides(shape = F, linetype = F) +
  theme_bw() +
  theme(axis.title.x = element_text(colour="grey40",size=22,
                                    angle=0,hjust=0.5,vjust=0,face="plain"),
        axis.title.y = element_text(colour="grey40",size=22,
                                    angle=90,hjust=0.5,vjust=0.5,face="plain"),
        axis.text.x = element_text(colour="grey20",size=18,
                                   angle=0,hjust=0.5,vjust=0,face="plain"),
        axis.text.y = element_text(colour="grey20",size=18,
                                   angle=0,hjust=0.5,vjust=0,face="plain"),
        plot.margin = unit(c(0.5,2,1,1), "cm"),
        strip.text.x = element_text(size = 18),
        panel.margin = unit(1, "lines"))
```

## Same plot but separate by stimuli

```{r}
ms_iChart_count <- ss_iChart_long %>% 
  group_by(Sub.Num, Time.ms, stimuli, value_cat) %>% 
  dplyr::summarise(count = ifelse(n() == 0, 0, n())) %>% 
  dplyr::summarise(sum_count = sum(count)) %>% 
  ungroup() %>% 
  mutate(Sub.Num = as.character(Sub.Num))

ms_iChart <- as.data.frame(xtabs(~ Sub.Num + value_cat + Time.ms, data = ss_iChart_long),
                           stringsAsFactors = F) %>% 
  left_join(y = ms_iChart_count, by = c("Sub.Num", "Time.ms")) %>% 
  mutate(proportion_looking = Freq / sum_count,
         stimuli_factor = as.factor(stimuli))

ms_mean_iChart <- ms_iChart %>% 
  group_by(Time.ms, stimuli, value_cat) %>% 
  langcog::multi_boot_standard(col = "proportion_looking", na.rm = T) %>% 
  filter(is.na(stimuli) == F)

breaks <- seq(0,3000, by=500)
y_breaks <- seq(0.25,1.0, by = 0.25)
points <- seq(0,3000, by = 200)

# change factor levels for plot to help compare conditions of interest
ms_mean_iChart$stimuli <- factor(ms_mean_iChart$stimuli, 
                                 levels = c("ASL", "Object", "Face", "Bullseye"))

ggplot(aes(x = as.numeric(Time.ms), y = mean, 
           color = value_cat), 
       data = ms_mean_iChart) + 
  ylim(0,1) +
  geom_line(data = filter(ms_mean_iChart, as.numeric(Time.ms) %in% points), size=0.7) +
  geom_linerange(data = filter(ms_mean_iChart, Time.ms %in% points),
                 aes(ymin=ci_lower, ymax=ci_upper), alpha = 0.5) +
  geom_point(data = filter(ms_mean_iChart, as.numeric(Time.ms) %in% points), size=2) +
  scale_color_brewer(palette = "Set1", type = "qual") +
  xlab("Time in ms from onset of noun") +
  ylab("Proportion looking") +
  guides(shape = F, linetype = F) +
  theme_bw() +
  facet_wrap(~stimuli, ncol = 2) +
  theme(axis.title.x = element_text(colour="grey40",size=18,
                                    angle=0,hjust=0.5,vjust=0,face="plain"),
        axis.title.y = element_text(colour="grey40",size=18,
                                    angle=90,hjust=0.5,vjust=0.5,face="plain"),
        axis.text.x = element_text(colour="grey20",size=12,
                                   angle=0,hjust=0.5,vjust=0,face="plain"),
        axis.text.y = element_text(colour="grey20",size=12,
                                   angle=0,hjust=0.5,vjust=0,face="plain"),
        plot.margin = unit(c(0.5,2,1,1), "cm"),
        strip.text.x = element_text(size = 16),
        panel.margin = unit(1, "lines"),
        legend.position = "top")
```

## Cluster-based permutation test of center looking curves

The cluster-based permutation test only tests differences between two curves. So in this chunk we can control which conditions to compare. 

```{r}
conds_to_compare <- c("ASL", "Face")
```

Filter the timeseries data to the analysis window.

```{r}
d_cluster <- ss_iChart_long %>% 
  rename(condition = stimuli, 
         t_stim = Time.ms_numeric,
         subid = Sub.Num,
          tr.num = Tr.Num,
         target_looking = value_cat) %>% 
  filter(t_stim >= 0, t_stim <= 2000, condition %in% conds_to_compare)  %>% 
  mutate(condition = as.character(condition))

# Create time bins for each trial and flag bins with participant data in both conditions.
d_cluster %<>% 
  group_by(subid, tr.num) %>% 
  arrange(t_stim) %>% 
  mutate(Bin = 1:n())
```

Aggregate prop looking data.

```{r}
# Aggregate at the subject level
ss.d <- d_cluster %>% 
  group_by(subid, Bin, condition) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  complete(nesting(subid, Bin), condition,
           fill = list(count = 0)) %>% 
  mutate(subid = as.character(subid))

# get proportion looking by dividing the freq looking at each target by the total looking 
# derived in step 2 of this chunk
ss.d <- as.data.frame(xtabs(~ subid + Bin + target_looking + condition, 
                            data = d_cluster),
                      stringsAsFactors = F) %>% 
  mutate(Bin = as.numeric(Bin)) %>% 
  left_join(x = ss.d, y = ., by = c("subid", "Bin", "condition")) %>% 
  mutate(proportion_looking = Freq / count)

# plot
ms.means.timeslice <- ss.d %>% 
  group_by(Bin, condition, target_looking) %>% 
  summarise(mean = mean(proportion_looking, na.rm = T))

ms.means.timeslice %>% 
  filter(Bin <= 60) %>% 
  ggplot(aes(x = as.numeric(Bin), y = mean, 
           color = target_looking, linetype = condition), data = .) + 
  lims(y = c(0,1), x = c(0, 65)) +
  geom_line(size=1, position = position_dodge(width = 0.1)) +
  guides(color = F) +
  xlab("Time in sec from onset of trial") +
  ylab("Proportion looking") +
  geom_dl(aes(label = target_looking), method = "last.bumpup") +
  labs(linetype = "condition: ") +
  theme(legend.position = "top") +
  scale_color_manual(values = c("dodgerblue", "red", "forestgreen"))
```

Rename variables to work with cluster-based permutation functions. We also create a varible to track complete cases for the paired t-test.

Note that we use Bin > 1 because the first bin everyone is looking at center, so we can't do a t-test because "data are constant."

```{r}
ss_filt <- ss.d %>% 
  filter(Bin > 1, Bin <= 60) %>% 
  filter(!is.na(proportion_looking),
         target_looking == "Center") %>% 
  mutate(trial_type = condition,
         ss_m = proportion_looking,
         Participant = subid) %>% 
  group_by(subid, Bin) %>% 
  mutate(n = n(),
         complete_case = ifelse(n == 2, "complete", "incomplete"))

ss_complete <- ss_filt
```

### Get clusters 

```{r}
# Define thresholds for t-test
num_sub = length(unique((ss_filt$subid)))
threshold_t = qt(p = 1 - .05 / 2, df = num_sub - 1) # pick threshold t based on alpha = .05 two tailed

# Nest the data by time bin 
by_bin <- ss_complete %>% 
  group_by(Bin) %>% 
  nest()

# Find and filter by the longest timepoint to make sure we have 
# the same number of data points to comapre across conditions
bin_cut_point <- ss_complete %>% 
  group_by(trial_type) %>% 
  summarise(max_bin = max(Bin)) %>% 
  pull(max_bin) %>% 
  min()

by_bin %<>% filter(Bin <= bin_cut_point, Bin > 1)
```

```{r}
# 1. Map the t-test to each time bin and store it as a variable in the by_bin data frame. 
by_bin %<>% mutate(model = map(data, run_t_test, paired = FALSE, alternative = "two.sided"))

# 2. Take the time-bins whose test passed the threshold statistic 
# (e.g., t > 2), and group them by adjacency. We will call these 
# time-clusters. To do this, we extract the t-stats and their 
# bin numbers and filter by the threshold defined above.

# get t-stats for each model
t_df <- by_bin %>% 
  mutate(glance = map(model, broom::glance)) %>% 
  unnest(glance, .drop = TRUE) %>% 
  select(Bin, statistic, p.value)

# filter by t threshold 
t_df_filt <- t_df %>% filter(abs(statistic) >= threshold_t)

# 3. For each time-cluster, calculate the sum of the 
# statistics for the time-bins inside it.
t_df_filt %<>% define_clusters()

# 4. Next, we calculate the sum of the statistics for the time-bins inside each cluster.
observed_t_vals <- t_df_filt %>% 
  group_by(cluster) %>% 
  summarise(sum_t = sum(statistic)) 

observed_t_vals %>% head()
```

Take the largest of the cluster-level statistics and store it for later. This is the t-value that we will use to compare against our null distribution generated by the permutations that we will create in the next part of the program.

```{r}
largest_t <- observed_t_vals %>% pull(sum_t) %>% abs() %>% max()
```

### Generate null distribution by shuffling data

We will take our data and randomly shuffle it, assigning participants randomly to each condition. Then we will perform steps (1)-(3) on the shuffled dataset and save the biggest sum-statistic from each permutation analysis. 

So we create a list of data frames. Each data frame is a permuted dataset.

```{r, message=F}
# global variables for setting up shuffling and 
set.seed(7)
n_permutations <- 1000
permuted_dfs <-  vector("list", length(1:n_permutations))

for (n_sim in 1:n_permutations) {
  permuted_dfs[[n_sim]] <- permute_data(ss_complete, within = F)
}

# Now that we have our list of permuted datasets, we map the statistical test 
# using a wrapper function to each time bin in each dataset. 
# *Note that this takes about 4.5 minutes to do 1000 permutations.*
# currently does not support paired T-tests

system.time(
  null_dist_1 <- map(permuted_dfs, t_test_by_bin, paired = F, 
                     alternative = "two.sided", 
                     bin_cut = bin_cut_point)
)

# Next, we need to extract the t-stats, filtering by the pre-defined threshold. 
# **Note that this takes about 4.5 minutes to do 1000 permutations.**
system.time(
  null_dist_2 <- map(null_dist_1, extract_t, t_threshold = threshold_t, alternative = "two.sided") 
)

# Then, we make clusters for each filtered data frame of Bins that have above-threshold t-values.
null_dist_3 <- map(null_dist_2, define_clusters)  

# Then, we sum the t-stats for each cluster and store the largest value for each simulation. 
null_dist_4 <- map(null_dist_3, sum_t_stats) %>% unlist() %>% data.frame(null_t = .)  

beepr::beep(sound = 3)
```

Now we have a data frame with a single column that is a distribution of t-stats from the randomly shuffled datasets. We can use this as a null distribution against which we can compare the cluster-statistic measured from our actual data in step (3). This will get us a p-value, or the probability of that we would have observed our data or something more extreme if the null were true.

We can visualize this null distribution along with our sample sum-statistic.

```{r}
null_dist_4 %>% 
  ggplot(aes(x = null_t)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = sum_t),
             data = observed_t_vals,
             color = "darkorange", 
             linetype = "dashed",
             size = 1) +
  theme_classic()
```

Finally, to compute the p-value we compare the cluster-statistic from step (3) to the distribution found in (6). 

```{r}
t_value_to_test <- largest_t

nulls <- null_dist_4 %>% 
  filter(!is.na(null_t)) %>% 
  mutate(sig = abs(null_t) >= abs(t_value_to_test)) %>% # test if the null-sum stat is greater than the the sum-stat for the largest cluster
  pull(sig)

crit_p <- ( sum(nulls) / length(nulls) ) %>% round(3) # get the p-value rounded to 3 digits
crit_p
```

Cluster-based permutation results:
  * Face vs. ASL p-value is p < .001
  * Face vs. Bullseye p-value is p < .001
  * No p-values greater than t-threshold for the Object vs. Bullseye comparison -- highly similar looking curves.
