---
title: "Adults and preschoolers seek visual information to support language comprehension in noisy environments"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
  \author{{\large \bf Kyle MacDonald}, {\large \bf Virginia Marchman}, {\large \bf Anne Fernald}, \and {\large \bf Michael C. Frank} \\ \{kylem4, marchman, afernald, mcfrank\} @stanford.edu \\ Department of Psychology, Stanford University}

abstract: 
    "Language comprehension in grounded contexts is a multisensory process in whcih listeners rapidly integrate information from both the visual and the linguistic signals. But how should we prioritize these different information sources? Here, we test the hypothesis that even young listeners will flexibly adapt the dynamics of gaze to seek higher value visual information by increasing visual fixations to a speaker when the auditory signal is less reliable. We measured the timing and accuracy of adults (n=31) and children's (n=40, 3-5 y.o.) eye movements during a real-time language comprehension task. We found that both age groups delayed the timing of gaze shifts away from a speaker's face when processing speech in the presence of background noise. This delay resulted in listeners gathering more information from the visual signal, which results in more accurate gaze shifts, and fewer random eye movements to the rest of the visual world. These results suggest that even young listeners respond to the demands of different processing contexts by adapting gaze patterns to support language comprehension."
    
keywords:
  "eye movements; language processing; information-seeking; speech in background noise; development"

output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/',echo=F, warning=F, cache=T, message=F, include =F,
                      sanitize = T)

library(here);
source(here::here("R/helper_functions/paper_helpers.R"))
source(here::here("R/helper_functions/ewma_helper_funs.R"))
```

```{r data paths}
data_path <- "data/3_final_merged_data/first_shifts/"
ewma_path <- here::here("data/3_final_merged_data/ewma_output/")
hddm_path <- "data/3_final_merged_data/hddm_output/"
```

```{r read data}
d_kids_noise <- read_csv(here::here(data_path, "speed_acc_child_noise_fstshift_tidy.csv")) 
d_kids_gaze <- read_csv(here::here(data_path, "speed_acc_child_gaze_fstshift_tidy.csv")) 
d_adults <- read_csv(here::here(data_path, "speed_acc_adult_ng_fstshift_tidy.csv")) 
```

```{r read ewma output}
noise_ewma_files <- c("speed_acc_kids_noise_ewma_results.csv",
                      "speed_acc_adult_ng_ewma_results.csv")

d_ewma_noise <- noise_ewma_files %>% 
  purrr::map_df(read_ewma, path = ewma_path) 
```

```{r read hddm output}
d_hddm <- read_csv(here::here(hddm_path, "hddm_tidy.csv"))
```

```{r read bda output}
d_models <- readRDS(here::here("data/3_final_merged_data/first_shifts", "speed-acc-posterior-samples.rds"))
```

```{r clean datasets for merge}
d_adults %<>% select(-age)

d_kids_noise %<>% mutate(gaze_condition = ifelse(gaze_condition == "no_gaze", 
                                                 "straight_ahead", 
                                                 gaze_condition))

d_kids_gaze %<>% mutate(noise_condition = ifelse(noise_condition == "no_noise", 
                                                 "clear", 
                                                 noise_condition))
```

```{r merge datasets}
d <- bind_rows(mutate(d_kids_noise, experiment = "kids_noise", age_category = "children"),
               mutate(d_kids_gaze, experiment = "kids_gaze", age_category = "children"),
               mutate(d_adults, experiment = "adults_ng", age_category = "adults")) %>% 
  select(-resp_onset_type_fact, -subid_short) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

# uncomment below to test that we have the right number of rows after the merge (result should be TRUE)
# nrow(d_kids_gaze) + nrow(d_kids_noise) + nrow(d_adults) == nrow(d)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-gaze-trial-info.csv"),
                        col_types = cols(.default = "c"))
d_noise_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-noise-trial-info.csv"),
                         col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-adult-ng-trial-info.csv"),
                            col_types = cols(.default = "c"))
```

```{r merge stim info}
d_stim <- bind_rows(mutate(d_noise_stim, experiment = "kids_noise"),
                    mutate(d_gaze_stim, experiment = "kids_gaze"),
                    mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

# Introduction

As skilled listeners, we continually integrate information from the visual and the linguistic signals to understand what others are saying. A classic demonstration of this integration process is the "McGurk effect" where a speaker's mouth movements suggest one sound while their acoustic output suggests another. This conflict results in the listener perceiving a third, intermediate sound [@macdonald1978visual]. Findings such as these have inspired prominent theories of speech perception [@mcclelland2006there] and lexical processing [@macdonald2006constraint; @smith2017multimodal] that argue for the importance of *interactive* processes -- where listeners integrate information from multiple sources in parallel. Moreover, empirical work on speech perception shows that adults are better able to recover linguistic information in noisy contexts when they have visual access to a speaker's face [@erber1969interaction]

However, the usefulness of integrating visual information varies depending on features of the listener and features of the processing context. Consider the familiar example of a friend who asks you to "Pass the salt" in a noisy restaurant. Here, comprehension could be facilitated by gathering visual information by allocating visual attention to the speaker to read her lips or the direction of her gaze. A second case study is the understanding a visual-manual language like American Sign Language (ASL). Here, the value of allocating visual fixations to the language source (i.e., the signer) is high since all of the language-relevant information is available in that location. 

In prior work, we showed that, compared to spoken language learners, ASL-learners will delay shifting gaze away from a language source until they have accumulated sufficient information to generate a highly-accurate eye movement [@macdonald2017info]. In contrast, spoken language learners were more likely to generate early, exploratory gaze shifts. We explained these differences using an information-seeking account: that listeners flexibly adapted the dynamics of gaze in response to the value of gathering visual information within a particular language processing context.

Our account represents a synthesis of ideas from several research programs. First, work on language-mediated visual attention shows that adults and children rapidly shift gaze upon hearing the name of an object in the visual scene [@allopenna1998tracking; @tanenhaus1995integration]. The speed and consistency of this visual response has led to debates about whether language-mediated gaze shifts are automatic as opposed to under the control of the listener. Second, empirical work on vision during natural tasks shows that people overwhelmingly prefer to look at *goal-relevant* locations -- e.g., an upcoming obstacle while walking [@hayhoe2005eye]. These accounts predict that gaze dynamics during language comprehension should adapt to the value of different fixation behaviors with respect to the listener's goal of language comprehension. Finally, work on "effortful listening" shows that listeners generate compensatory responses (e.g., increases in attention and working memory) within "challenging" comprehension contexts such as processing noisy or accented speech [@van2014listening]. These accounts predict that young listeners might compensate for the reduced quality of the auditory signal by allocating gathering additional visual information.

Here, we test the generality of our information-seeking account of eye movements during grounded language comprehension. We ask whether listeners will adapt the timing of gaze shifts away from a speaker when the auditory signal becomes less reliable -- as is the case when processing speech in noisy environments. Recent evidence suggests that gaze during lexical access can adapt to the demands of different processing contexts. For example, recent work by @mcmurray2017waiting shows that individuals with Cochlear Implants, who are consistenly processing degraded auditory input, are more likely to delay the process of lexical access as measured by slower gaze shifts to named referents and fewer incorrect gaze shifts to phonological onset competitors, as compared to listeners with typical hearing. @mcmurray2017waiting also found that they could replicate these changes in lexcial access in adults with typical hearing by degrading the auditory stimuli in a way that shares features with the output of a cochlear implant (noise-vocoded speech).

```{r stimuli_plot, include = T, fig.env = "figure*", fig.pos = "tb", fig.width=5, fig.asp = 0.4, out.width= "70%", fig.align='center', fig.cap = "Stimuli information. Panel A shows the timecourse of the linguistic stimuli for a single trial. Panel B shows the layout of the three fixation locations (speaker, target, and distracter). And panel C shows a visual representation of the clear and noisy waveforms used in E1."}
grid::grid.raster(png::readPNG(here::here("paper/cogsci2018/figs/stimuli_info.png")))
```

A second goal of this work is to test whether children would show a similar pattern of flexibly adapting fixation behaviors in response to changes in the utility of gathering certain kinds of visual information. Recent developmental work shows that, like adults, preschoolers will flexibly adjust how they interpret ambiguous sentences (e.g., "I had carrots and *bees* for dinner.") by integrating information about the relibility of the incoming perceptual information with their expectations about the speaker [@yurovsky2017preschoolers]. While children's behavior showed impressive parallels to adults, they relied more on top-down expectations about the speaker perhaps because there was more perceptual noise compared to adults. These developmental findings provide insight into how children succeed in efficient language comprehension despite having partial knowledge of word-object links and a fully-developed internal language model.

Here, we hypothesized that a noisy auditory environment creates a scenario where the auditory signal becomes less reliable, and in turn increases the value of fixating on a speaker for the task of language understanding. Our key behavioral prediction is that listeners in noisy contexts will delay generating an eye movement away from a speaker until they have accumulated additional visual information about the identity of the named referent.  We also predicted that preschoolers would show a parallel pattern of adaptation to noisy contexts and allocate more fixations to a speaker's face when it became more useful for maintaining accurate language comprehension. To quantify evidence for our predictions, we analyze the Accuracy and Reaction Times (RTs) of listeners' first gaze shifts after hearing the name of an object in the visual scene. We focus on first shifts because they provide a window onto changes in the underlying dynamics of decision processes that generate eye movements. However, it is important to point out that when we analyze differences in accuracy, we are not making claims about the overall amount of time spent looking at the target vs. the distractor image -- a measure typically used in analyses of the Visual World Paradigm.

# Experiment

In this experiment, we recorded adults and children's eye movements during a real-time language comprehension task where participants processed familiar sentences (e.g., "Where's the ball?") while looking at a simplified visual world with three fixation targets (see Fig 1). Using a within-participants design, we manipulated the signal-to-noise ratio of the auditory signal by convolving the auditory input with brownian noise (i.e., random noise patterns). 

First, we present standard behavioral analyses of Reaction Time (RT) and accuracy of listeners' first gaze shifts after target noun onset. Then, we present two model-based analyses that link observable behavior to underlying psychological constructs. (1) We use an exponentially weighted moving average (EWMA) method [@vandekerckhove2007fitting] to classify participants' gaze shifts as language-driven or random. In contrast to the standard RT/Accuracy analysis, the EMWA approach allows us to quantify participants' willingness to generate gaze shifts after noun onset but before collecting sufficient information to seek the named referent. Higher values indicate that participants were shifting early and equally likely to land on the target or distractor image. (2) We use drift-diffusion models (DDMs) [@ratcliff2015individual] to ask whether behavioral differences in Accuracy and RT are driven by a more cautious responding strategy or by more efficient information processing -- an important distinction for our theoretical account. 

We predicted that processing speech in noisy contexts would make participants less likely to shift before collecting sufficient information, which in turn would lead to a lower proportion of shifts flagged as random in the EWMA analysis, and a pattern of DDM results that indicates a prioritization of accuracy over and above speed (see the Analysis Plan section below for more details on the models). We also predicted both (a) developmental differences: that children would produce a higher proportion of random shifts and accumulate information less efficiently compared to adults, and (b) developmental parallels: that children would show an adult-like pattern of behavioral/model-based results in the noisy vs. clear processing contexts.

```{r noise_acc_rt_e1_plot, include = T, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.7, out.width= "85%", fig.align='center', fig.cap = "Behavioral results for first shift Reaction Time (RT) and Accuracy. Panel A shows violin plots representing the distribution of RTs for each participant in each condition. Each point represents a participant's average RT. Color represents processing condition. The grey insets show the full posterior distribution of the plausible RT differences across conditions with the vertical dashed line representing the null value of zero condition difference. The green shading represents estimates in the predicted direction and above the null value while the red shading represents estimates below the null. Panel B shows the same information but for first shift accuracy."}
png::readPNG(here::here("paper/cogsci2018/figs/e1_behav_results.png")) %>% 
  grid::grid.raster()
```

## Method

### Participants

```{r e1 filter}
d_e1 <- d %>% 
  filter(experiment != "kids_gaze",
         keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         gaze_condition == "straight_ahead")
```

```{r noise participants}
e1_adults <- d_e1 %>% 
  filter(age_category == "adults") %>% 
  select(subid, gender) %>% 
  unique() %>%
  group_by(gender) %>% 
  tally()

e1_kids <- d_e1 %>% 
  filter(age_category == "children") %>% 
  select(subid, age, gender) %>% 
  unique() %>% 
  group_by(gender) %>% 
  tally()

n_adults_run <- d %>% 
  filter(age_category == "adults") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_run <- d %>% 
  filter(age_category == "children", experiment == "kids_noise") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_filt <- n_kids_run - sum(e1_kids$n)
n_adults_filt <- n_adults_run - sum(e1_adults$n)
```

Participants were native, monolingual English-learning children ($n=$ `r sum(e1_kids$n)`; `r e1_kids$n[1]` F, `r e1_kids$n[2]` M) and adults ($n=$  `r sum(e1_adults$n)`; `r e1_adults$n[1]` F, `r e1_adults$n[2]` M). All participants had no reported history of developmental or language delay and normal vision. `r n_kids_filt + n_adults_filt` participants (`r n_kids_filt` children, `r n_adults_filt` adults) were run but not included in the analysis because either the eye tracker falied to calibrate or the participant did not complete the task. 

### Stimuli 

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 2)) 
```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where’s the (target word) -- see panel A of Fig 1. The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=2)` ms. 

*Noise manipulation*. To create the stimuli in the noise condition, we convolved each recording with Brown noise using the Audacity audio editor. The average signal-to-noise ratio ^[The ratio of signal power to the noise power, with values greater than 0 dB indicating more signal than noise.] in the noise condition was 2.87 dB compared to the clear condition, which was 35.05 dB. 

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distractor image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of the target picture was counterbalanced across trials.

### Design and procedure

<!---
Participants viewed the ASL task on a 27" monitor. Children sat on their caregiver’s lap, and the child’s gaze was recorded using a digital camcorder set up behind the monitor. On each trial, pictures of two familiar objects appeared on the screen, a target object corresponding to the target noun, and a distracter object matched for visual salience. Between the two pictures was a central video of an adult female signing the name of one of the pictures. Participants saw 32 test trials with five filler trials (e.g. “YOU LIKE PICTURES? MORE WANT?”) interspersed to maintain children’s interest.

Participants viewed the Face, Object, and Bullseye tasks on a large projector screen in a sound-treated testing booth. Similar to the ASL task, at the beginning of each test trial, pictures of two familiar objects appeared on the screen and then a center stimulus appeared between the two pictures. The center stimulus varied across the three tasks: Face, Object, and Bullseye (see Figure 1 for details). Participants saw approximately 32 test trials with several filler trials interspersed to maintain children’s interest.

*Trial structure.* On each trial, the child saw two images of familiar objects on the screen for two seconds before the center stimulus appeared. This time allowed the child to visually explore both images. Next, the target sentence -- which consisted of a carrier phrase, target noun, and question sign -- was presented, followed by two seconds without language to allow the child to respond to the signer's sentence. The trial structure of the Face, Object, and Bullseye tasks were highly similar: children were given two seconds to visually explore the objects prior to the appearance of the center stimulus, then processed a target sentence, and finally were given two seconds of silence to generate a response to the target noun.

*Coding.* Participants’ gaze patterns were videotaped and later coded frame-by-frame at 33-ms resolution by trained coders blind to target side.  On each trial, coders indicated whether the eyes were fixated on the central signer, one of the images, shifting between pictures, or away (off), yielding a high-resolution record of eye movements aligned with target noun onset. Prior to coding, all trials were pre-screened to exclude those few trials on which the participant was inattentive or there was external interference.
-->

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 60 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared (see Fig 1). Next, they processed the target sentence -- which consisted of a carrier phrase, a target noun, and a question -- followed by two seconds without language to allow for a response. Child participants saw 32 trials (16 noise trials; 16 clear trials) with several filler trials interspersed to maintain interest. Adult participants saw 64 trials (32 noise; 32 clear). The noise manipulation was presented in a blocked design with the order of block counterbalanced across participants.

<!--
We excluded RTs longer than two seconds since these shifts are unlikely to be generated in response to the incoming language stimulus (see Ratcliff, 1993). 

This measure reflects the accuracy of language-driven saccades to the visual world. Mean first shift accuracy scores were computed for each participant for both correct and incorrect shifts. Trials where the participants did not generate a shift were not included in the computation.
-->

## Results and Discussion

```{r ewma_violin_plot, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.4", out.width = "80%", fig.align='center', fig.cap = "EWMA results for children and adults. Each point represents the proportion of shifts categorized as language-driven for a single participant. Color of represents the processing context: noise vs. clear."}

d_ewma_noise %>% 
  mutate(age_code = ifelse(str_detect(age_code, "child"), "children", "adults"),
         gaze_condition = ifelse(str_detect(condition, "gaze"), "gaze", 
                                 "straight_ahead"),
         noise_condition = ifelse(str_detect(condition, "noise"), "noise", "clear")) %>%
  filter(gaze_condition == "straight_ahead", 
         rt <= 2) %>% 
  group_by(subid, noise_condition, age_code, guess) %>% 
  summarise(count = n()) %>% 
  mutate(prop.responding = round(count / sum(count), 2)) %>% 
  filter(guess == "response") %>%
  ggplot(aes(x = fct_rev(age_code), y = prop.responding, color = noise_condition)) +
  #geom_line(aes(group = subid), color = "grey", alpha = 0.5) +
  geom_jitter(alpha = 0.8, shape = 21, 
              position = position_jitterdodge(jitter.width = 0.1, 
                                              dodge.width = 1)) +
  geom_violin(draw_quantiles = 0.5, trim = T, width = 1, size = 0.8, 
              adjust = 1, fill = NA) + 
  guides(fill = F) +
  lims(y = c(0,1)) +
  ggthemes::scale_color_ptol() +
  scale_x_discrete(expand = c(0,0.8), drop = FALSE) +
  labs(x = NULL, y = "Prop. Language-Driven Shifts", color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        text = element_text(size = 10),
        legend.title = element_text(size = 9)) 
```

```{r hddm_plot_noise, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.4", out.width = "85%", fig.align='center', fig.cap = "HDDM results. Each panel shows the posterior distribution for either the boundary separation or drift rate parameters for children (top panels) and adults (bottom panels)."}

d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("gaze_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         age_code = ifelse(age_code == "kid", "children", "adults"),
         age_code = factor(age_code, levels = c("children", "adults"))) %>% 
  ggplot(aes(x = param_value, color = condition)) +
  geom_line(stat = "density", size = 1, position = position_dodge(width=0.1)) +
  ggthemes::scale_color_ptol() +
  facet_grid(age_code~param_name, scales = "free") +
  labs(x = "Parameter Estimate", y = NULL, color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        axis.ticks = element_blank(), 
        axis.text.y = element_blank(),
        text = element_text(size = 10),
        legend.title = element_text(size =9)) 
```


### Analysis plan

First, we present behavioral analyses of First Shift Accuracy and Reaction Time (RT) ^[See https://osf.io/g8h9r/ for a pre-registration of the analysis plan.]. RT corresponds to the latency to shift away from the central stimulus to either picture measured from the onset of the target noun. (all RTs were analyzed in log space). Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter picture. We used the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and a within-participants manipulation -- by including random intercepts for each participant and item, and a random slope for each item and noise condition. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI). The HDI provides a range of credible values given the data and model. All analysis code can be found in the online repository for this paper: https://github.com/kemacdonald/speed-acc/blob/master/paper/cogsci2018/README_cogsci2018.md. 

Next, we present the two model-based analyses -- the EWMA and DDM. The goal of these models is to move beyond a description of the data and map behavioral differences in eye movements to underlying psychological variables. The EWMA method models changes in random shifting behavior as a function of RT. For each participant, the model classifies the proportion of shifts that were likely to be language-driven as opposed to random responding, which we call the *guessing* parameter. 

After we fit the EWMA, we took shifts categorized as language-driven and fit a hierarchical Bayesian drift-diffusion model (HDDM). This model quantifies differences in separable parameters of the underlying decision process that lead to different patterns of behavior. The model assumes that people accumulate noisy evidence in favor of one alternative with a response generated when the evidence crosses a pre-defined decision threshold. Here, we focus on two parameters of interest: **boundary separation**, which indexes the amount of evidence gathered before generating a response (higher values suggest more cautious responding) and **drift rate**, which indexes the amount of evidence accumulated per unit time (higher values suggest more efficient processing). 

```{r noise filter}
d_e1_analysis <- d_e1 %>% 
  filter(rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center") %>% 
  mutate(shift_acc_num = ifelse(shift_accuracy_clean == "correct", 1, 0),
         log_rt = log(rt))
```

```{r noise summarize model output}
ms_acc_e1 <- d_models$acc_noise %>% 
  group_by(noise_condition, age_category) %>% 
  summarise(prop = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

ms_rt_e1 <- d_models$rt_noise %>% 
  group_by(noise_condition, age_category) %>% 
  mutate(rt_ms_scale = rt_ms_scale * 1000) %>% 
  summarise(mean = median(rt_ms_scale),
            hdi_lower = quantile(rt_ms_scale, probs = 0.025),
            hdi_upper = quantile(rt_ms_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 
```

```{r noise create contrasts}
noise_contrast <- d_models$acc_noise %>% 
  select(sample_id:acc_prob_scale, -param_est) %>% 
  spread(noise_condition, acc_prob_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)

noise_contrast_rt <- d_models$rt_noise %>% 
  select(sample_id:rt_ms_scale, -param_est) %>% 
  spread(noise_condition, rt_ms_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)
```

### Behavioral analyses:

```{r ms contrast noise rt}
ms_noise_con_rt <- noise_contrast_rt %>%
  mutate(noise_contrast = noise_contrast * 1000) %>% 
  summarise(m_rt = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

**RT.** To make RTs more suitable for modeling on a linear scale, we analyzed responses in log space with the final model specified as: \texttt{$log(RT) \sim noise\_condition + age\_group + (sub\_id + noise\_condition \mid item)$}. Panel A of Figure 2 shows the full RT data distribution, the estimates of condition means, and the full posterior distribution of the estimated difference between the noise and clear conditions. Both children and adults were slower to identify the target in the noise condition (Children $M_{noise}$ = `r ms_rt_e1$m_rt[4]` ms; Adult $M_{noise}$ = `r ms_rt_e1$m_rt[3]` ms), as compared to the clear condition (Children $M_{clear}$ = `r ms_rt_e1$m_rt[2]` ms; Adult $M_{clear}$ = `r ms_rt_e1$m_rt[1]` ms). RTs in the noise condition were `r ms_noise_con_rt$m_rt[1]` ms slower on average, with a 95% HDI from `r ms_noise_con_rt$hdi_lower[1]` ms to `r ms_noise_con_rt$hdi_upper[1]` ms that did not include the null value of zero condition difference.

```{r ms contrast noise acc}
ms_noise_con_acc <- noise_contrast %>%
  summarise(m_acc = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

## hypothesis test
prob_diff0 <- noise_contrast %>% 
  summarise(prob = mean(noise_contrast >= 0)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

**Accuracy.** Next, we modeled adults and children's first shift accuracy using a mixed-effects logistic regression with the same specifications (see Panel B of Fig 2). Both groups were more accurate than a model of random responding (null value of $0.5$ falling well outside the lower bound of the 95% HDI for all group means). Adults were more accurate ($M_{adults} =$ `r ms_acc_e1$prop[1] * 100`%) than children ($M_{children} =$ `r ms_acc_e1$prop[2] * 100`%). The key result is that both groups showed evidence of higher accuracy in the noise condition: children ($M_{noise}$ = `r ms_acc_e1$prop[4]* 100`%; $M_{clear}$ = `r ms_acc_e1$prop[2]* 100`%) and adults ($M_{noise}$ = `r ms_acc_e1$prop[3]* 100`%; $M_{clear}$ = `r ms_acc_e1$prop[1]* 100`%). Accuracy in the noise condition was on average `r ms_noise_con_acc$m_acc[1] * 100`%  higher, with a 95% HDI from `r ms_noise_con_acc$hdi_lower[1]* 100`% to `r ms_noise_con_acc$hdi_upper[1] * 100`%. Note that the null value of zero difference falls at the very edge of the HDI such that `r prob_diff0$prob[1] * 100`% of the credible values fall below the null, providing evidence for higher accuracy in the noise condition.

### Model-based analyses:

```{r noise ewma group means summary}
# summarise group means for cutoffs
ms_cuts_noise <- d_models$ewma_cuts_noise %>% 
  group_by(age_category, noise_condition) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(model = "EWMA", parameter = "cut point") %>% 
  select(model, parameter, everything())

# summarise group means for prop guessing parameter
ms_guess_noise <- d_models$ewma_guess_noise %>% 
  group_by(age_category, noise_condition) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate(model = "EWMA", parameter = "prob. guessing") %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  select(model, parameter, everything())
```

```{r ewma cond difference summary}
## cut point model noise
ms_cond_diff_cuts <- d_models$ewma_cuts_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  rename(`age group` = age_beta, noise = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing <- d_models$ewma_guess_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`adults-children` = age_beta, 
         `noise-clear` = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## cut point model gaze
ms_cond_diff_cuts_gaze <- d_models$ewma_cuts_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  rename(`age group` = age_beta, gaze = straightahead_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing_gaze <- d_models$ewma_guess_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`gaze-straight_ahead` = straightahead_beta, 
         `adults-children` = age_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)
```


```{r ewma results table, results="asis", include = T}
ewma_table <- bind_rows(ms_cond_diff_cuts, ms_cond_diff_guessing) %>% 
  select(-Exp.)

ewma_table$Parameter[2] <- ""
ewma_table$Parameter[4] <- ""

ewma_table <- xtable::xtable(ewma_table, 
                             caption = "EWMA results for E1 and E2. The guessing parameter refers to the proportion of gaze shifts classified as random vs. language-driven with higher values indicating more random responding. Estimate refers to the difference between condition or age group.",
                             align = "lllr")

xtable::print.xtable(ewma_table, type="latex", comment = F, table.placement = "t",
      floating.environment = "table", 
      include.rownames=FALSE)
```

**EWMA.** Figure 3 shows the proportion of shifts that the model classified as random vs. language-driven for each age group and processing contxt. Critically, processing speech in noise caused both adults and children to produce a higher proportion of language-driven shifts with the 95% HDI excluding the null value (see Table 1). This pattern suggests that the noise condition led participants to increase visual fixations to the language source, leading them to generate fewer exploratory, random shifts before accumulating sufficient information to respond accurately.

**HDDM.** Figure 4 shows the full posterior distributions for the HDDM output. Children had lower drift rates and boundary separation estimates as compared to adults, suggesting that children were less efficient and less cautious in their responding (see also Table 2). The noise manipulation only affected the boundary separation parameter, with higher estimates in the noise condition for both age groups. This result suggests that participants' in the noise condition prioritized information accumulation over speed when generating an eye movement in response to the incoming language. This increased decision threshold led to higher accuracy. Moreover, the high overlap in estimates of drift rate suggests that participants were able to integrate the visual and auditory signals such that they could achieve a level of processing efficiency comparable to the clear processing context.

Together, the behavioral and EWMA/HDDM results provide converging support for the predictions of our information-seeking account. Processing speech in noise caused listeners to seek additional visual information to support language comprehension. Moreover, we observed a strikingly similar pattern of behavior in children and adults, with both groups producing more language-driven shifts and prioritizing accuracy over speed in the more challenging, noisy context. 

```{r print hddm table, include = T, results = "asis"}
hddm_table <- d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("straight_ahead_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(Condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         `Age Group` = ifelse(str_detect(age_code, "kid"), "children", "adults")) %>% 
  group_by(param_name, Condition, `Age Group`) %>% 
  summarise(MAP = mean(param_value),
            HDI_lower = quantile(param_value, probs = 0.025),
            HDI_upper = quantile(param_value, probs = 0.975)) %>% 
  rename(Parameter = param_name) %>% 
  mutate_at(vars(MAP, HDI_lower, HDI_upper), round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(`Estimate [95% HDI]` = paste0(MAP, " [", HDI_lower, ", ", HDI_upper, "]")) %>% 
  select(-MAP, -HDI_lower, -HDI_upper)

# remove double entries in the table
hddm_table$Parameter[1] <- ""
hddm_table$Parameter[3] <- ""
hddm_table$Parameter[4] <- ""

hddm_table$Parameter[5] <- ""
hddm_table$Parameter[7] <- ""
hddm_table$Parameter[8] <- ""

# conditions
hddm_table$Condition[2] <- ""
hddm_table$Condition[4] <- ""
hddm_table$Condition[6] <- ""
hddm_table$Condition[8] <- ""

# now make the xtable
hddm_table <- xtable::xtable(hddm_table, 
                             caption = "HDDM parameter estimates for each age group and noise condition. The drift rate parameter indexes processing efficiency and the boundary separation parameter indexes participants' information accumulation threshold.",
                             align = "llllr")

hlines <- c(-1, 0, 4, nrow(hddm_table))

xtable::print.xtable(hddm_table, type="latex", comment = F, table.placement = "t",
      hline.after = hlines,
      floating.environment = "table", 
      include.rownames=FALSE)
```


# General Discussion

Language comprehension in grounded contexts involves integrating the visual and linguistic signals. But the value of gathering visual information can vary depending on features of the processing context. Here, we presented a test of an information-seeking account of eye movements during language processing -- an account that we first proposed in @macdonald2017info to explain population-level differences in the dynamics of gaze between children learning ASL and children learning spoken English. Here, we showed that children and adults adapt to processing speech in noise by producing slower but more accurate gaze shifts away from a speaker. Both groups also showed evidence of prioritizing information accumulation over speed (HDDM) while producing more language driven shifts (EWMA). It is interesting that listeners were able to achieve higher accuracy in the more challenging, noisy context. Together, the behavioral and modeling results suggest that when the linguistic signal is degraded, listeners adapt their eye movements to seek language-relevant information in the visual world.

These results bring together ideas from several research programs. First, work on language-mediated visual attention shows that adults and children rapidly shift gaze upon hearing the name of an object in the visual scene [@allopenna1998tracking; @tanenhaus1995integration]. The speed and consistency of this response has led to debates about whether language-mediated gaze shifts are automatic as opposed to under the control of the listener. While we do claim that listeners in our task have explicit access to the underlying decision process, our findings show that the dynamics of gaze during lexical access adapt to the information features of the context. This finding parallels recent work by @mcmurray2017waiting, showing that adults with Cochlear Implants, who consistently process degraded auditory input, will delay the process of lexical access, waiting to begin until substantial information has accumulated.  

Second, empirical work on vision during natural tasks shows that people overwhelmingly prefer to look at *goal-relevant* locations -- e.g., an upcoming obstacle while walking [@hayhoe2005eye]. These accounts inspired our prediction that gaze dynamics during language comprehension should adapt to the value of different fixation behaviors with respect to the listener's goal of rapid language processing. And third, work on effortful listening shows that listeners generate compensatory responses (e.g., increases in attention and working memory) within "challenging" comprehension contexts such as processing noisy or accented speech [@van2014listening]. These accounts predict that our young listeners might compensate for the reduced quality of the auditory signal by allocating gathering additional visual information.

This work has several important limitations that we hope will pave the way for future work. Here, we chose to focus on a single decision about visual fixation to provide a window onto the underlying dynamics of decision-making across different processing contexts. However, the decision to shift away from a language is just one of the many decisions that listeners make while processing language in real-time. Moreover, our analysis does not consider the rich information present in the gaze patterns that occur leading up to this decision. In our future work, we aim to quantify changes in the dynamics of gaze across the full sentence processing context. Finally, we used a simple visual world, with only three places to look, and very simple linguistic stimuli, especially for the adults. Thus it remains an open question how these results might scale up to more realistic language environments.

We designed this experiment to test the generalizability of our information-maximization proposal within the domain of familiar language comprehension. However, we think that the account is more general. And we are interested in applying this framework -- in-depth analysis of decisions about visual fixation -- to the language acquisition context. Consider that early in language learning children are acquiring novel word-object links while also learning about visual object categories. Both of these tasks produce goals that should in turn modulate children's decisions about where to allocate visual attention, e.g., seeking nonlinguistic cues to reference such as eye gaze and pointing become critical when you are unfamiliar with the information in the linguistic signal. More generally, we think that these results contribute to a recent theoretical emphasis on including goal-based accounts of eye movements during language comprehension [@salverda2011goal]. We hope that our approach presents a way forward for explaining fixation behaviors across a wider variety of populations, processing contexts, and during different stages of language learning.

```{r session info, include = F}
sessionInfo() %>% pander::pander(compact = F)
```

# Acknowledgements

We are grateful to the families who participated in this research. Thanks to Tami Alade and Hannah Slater for help with data collection. This work was supported by an NSF GRFP to KM.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
