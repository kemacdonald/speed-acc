---
title: "Adults and preschoolers seek visual information to support language comprehension in noisy environments"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
  \author{{\large \bf Kyle MacDonald}, {\large \bf Virginia Marchman}, {\large \bf Anne Fernald}, \and {\large \bf Michael C. Frank} \\ \{kylem4, marchman, afernald, mcfrank\} @stanford.edu \\ Department of Psychology, Stanford University}

abstract: 
    "Language comprehension in grounded, social contexts involves integrating information from both the visual and the linguistic signals. But how should listeners prioritize these different information sources? Here, we test the hypothesis that even young listeners flexibly adapt the dynamics of their gaze to seek higher value visual information when the auditory signal is less reliable. We measured the timing and accuracy of adults (n=31) and 3-5 year-old children's (n=39) eye movements during a real-time language comprehension task. Both age groups delayed the timing of gaze shifts away from a speaker's face when processing speech in a noisy environment. This delay resulted in listeners gathering more information from the visual signal, more accurate gaze shifts, and fewer random eye movements to the rest of the visual world. These results provide evidence that even young listeners adjust to the demands of different processing contexts by seeking out visual information that supports language comprehension."
    
keywords:
  "eye movements; language processing; information-seeking; speech in background noise; development"

output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/',echo=F, warning=F, cache=T, message=F, include =F,
                      sanitize = T)
```

```{r libaries}
library(here);
source(here::here("R/helper_functions/paper_helpers.R"))
source(here::here("R/helper_functions/ewma_helper_funs.R"))
```

```{r data paths}
data_path <- "data/3_final_merged_data/first_shifts/"
ewma_path <- here::here("data/3_final_merged_data/ewma_output/")
hddm_path <- "data/3_final_merged_data/hddm_output/"
bda_path <- "data/3_final_merged_data/bda_posterior_samples/"
```

```{r read data}
d_kids_noise <- read_csv(here::here(data_path, "speed_acc_child_noise_fstshift_tidy.csv")) 
d_kids_gaze <- read_csv(here::here(data_path, "speed_acc_child_gaze_fstshift_tidy.csv")) 
d_adults <- read_csv(here::here(data_path, "speed_acc_adult_ng_fstshift_tidy.csv")) 
```

```{r read ewma output}
noise_ewma_files <- c("speed_acc_kids_noise_ewma_results.csv",
                      "speed_acc_adult_ng_ewma_results.csv")

d_ewma_noise <- noise_ewma_files %>% 
  purrr::map_df(read_ewma, path = ewma_path) 
```

```{r read hddm output}
d_hddm <- read_csv(here::here(hddm_path, "hddm_tidy.csv"))
```

```{r read bda output}
d_models <- readRDS(here::here(bda_path, "speed-acc-noise-posterior-samples.rds"))
```

```{r clean datasets for merge}
d_adults %<>% select(-age)

d_kids_noise %<>% mutate(gaze_condition = ifelse(gaze_condition == "no_gaze", 
                                                 "straight_ahead", 
                                                 gaze_condition))

d_kids_gaze %<>% mutate(noise_condition = ifelse(noise_condition == "no_noise", 
                                                 "clear", 
                                                 noise_condition))
```

```{r merge datasets}
d <- bind_rows(mutate(d_kids_noise, experiment = "kids_noise", age_category = "children"),
               mutate(d_kids_gaze, experiment = "kids_gaze", age_category = "children"),
               mutate(d_adults, experiment = "adults_ng", age_category = "adults")) %>% 
  select(-resp_onset_type_fact, -subid_short) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

# uncomment below to test that we have the right number of rows after the merge (result should be TRUE)
# nrow(d_kids_gaze) + nrow(d_kids_noise) + nrow(d_adults) == nrow(d)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-gaze-trial-info.csv"),
                        col_types = cols(.default = "c"))
d_noise_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-noise-trial-info.csv"),
                         col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-adult-ng-trial-info.csv"),
                            col_types = cols(.default = "c"))
```

```{r merge stim info}
d_stim <- bind_rows(mutate(d_noise_stim, experiment = "kids_noise"),
                    mutate(d_gaze_stim, experiment = "kids_gaze"),
                    mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

# Introduction

When processing language, we integrate information from the visual and linguistic signals to understand what others are saying. A classic demonstration of this integration is the "McGurk effect" where a speaker's mouth movements suggest one sound while their acoustic output indicates another. This conflict results in the listener perceiving a third, intermediate sound [@macdonald1978visual]. Findings such as these have inspired prominent theories of speech perception [@mcclelland2006there] and lexical processing [@macdonald2006constraint] that argue for the importance of *interactive* processes -- where listeners integrate information from multiple sources in parallel. Moreover, empirical work on speech perception shows that adults are better able to recover linguistic information in noisy contexts when they have visual access to a speaker's face [@erber1969interaction].

But how should listeners prioritize different kinds of information? Consider that the value of integrating visual information can change depending on features of the listener and the processing context. For example, if a friend asks you to "Pass the salt" in a noisy restaurant, you could facilitate comprehension by looking to the speaker's face to read her lips or perhaps the direction of her gaze. A second case is the comprehension of a visual-manual language, e.g., American Sign Language (ASL). Here, the value of allocating visual fixations to the language source (the signer) is high since all of the language-relevant information is available in that location. 

In prior work, we showed that, compared to spoken language learners, ASL-learners delay shifting gaze away from a language source until they have accumulated sufficient information to generate highly-accurate eye movements [@macdonald2017info]. In contrast, spoken language learners were more likely to produce early, random gaze shifts when seeking named referents. We explained these differences using an information-seeking account: that listeners flexibly adapted the dynamics of their gaze in response to contexts where the value of gathering visual information was high.

Our account was inspired by ideas from several research programs. First, work on language-mediated visual attention shows that adults and children rapidly shift gaze upon hearing the name of an object in the visual scene [@allopenna1998tracking; @tanenhaus1995integration]. Second, empirical work on visual attention during everyday tasks shows that people overwhelmingly prefer to look at *goal-relevant* locations -- e.g., an upcoming obstacle while walking [@hayhoe2005eye]. Finally, work on "effortful listening" shows that people will generate compensatory responses (e.g., increases in attention and working memory) within "challenging" language contexts such as processing noisy or accented speech [@van2014listening]. Together, these accounts predict that gaze dynamics during language comprehension should adapt to compensate for the reduced quality of the auditory signal and to facilitate the listener's goal of comprehension.

```{r stimuli_plot, include = T, fig.env = "figure*", fig.pos = "tb", fig.width=5, fig.asp = 0.3, out.width= "75%", fig.align='center', fig.cap = "Experimental design and stimuli. Panel A shows the timecourse of the linguistic stimuli for a single trial. Panel B shows the layout of the three fixation locations (speaker, target, and distracter). Panel C shows a visual representation of the clear and noisy waveforms."}
grid::grid.raster(png::readPNG(here::here("paper/cogsci2018/figs/stimuli_info.png")))
```

Here, we synthesize these ideas and test the generality of our information-seeking account of eye movements during grounded language comprehension. We ask whether listeners will adapt the timing of gaze shifts away from a speaker if the auditory signal is less reliable -- as is the case when processing speech in a noisy environment.  

The second goal of this work is to test whether children show a similar pattern of behavior and flexibly adapt fixations in response to changes in the utility of gathering certain kinds of visual information. Recent developmental work shows that, like adults, preschoolers will flexibly adjust how they interpret ambiguous sentences (e.g., "I had carrots and *bees* for dinner.") by integrating information about the reliability of the incoming perceptual information with their expectations about the speaker [@yurovsky2017preschoolers]. While children's behavior paralleled adults, they relied more on top-down expectations about the speaker, perhaps because their developing perceptual representations were noisier compared with adults. These developmental differences provide insight into how children succeed in understanding language despite having partial knowledge of word-object links and without a fully-developed language model.

In our experiment, we hypothesized that a noisy auditory environment increases the value of fixating a speaker to gather visual information that supports comprehension. Our key behavioral prediction is that listeners in noisy contexts will delay generating an eye movement away from a speaker until they have accumulated additional visual information about the identity of the named referent. This delay, in turn, will lead to fewer random gaze shifts to the rest of the visual world.  We also predicted that preschoolers would show a parallel pattern of adaptation to noisy contexts and allocate more fixations to a speaker's face when it became more useful for accurate language comprehension. A plausible alternative to our hypothesis is that the effects of language on visual attention are so well-practiced that we would not see listeners adapt their gaze patterns to the processing context.

To quantify the evidence for our predictions, we analyze the accuracy and reaction times (RTs) of listeners' first gaze shifts after hearing the name of an object in the visual scene. We focus on first shifts because they provide a window onto changes in the underlying dynamics of decision processes that generate eye movements.  

# Experiment

In this experiment, we recorded adults and children's eye movements during a real-time language comprehension task where participants processed familiar sentences (e.g., "Where's the ball?") while looking at a simplified visual world with three fixation targets (see Fig.\ \ref{fig:stimuli_plot}). Using a within-participants design, we manipulated the signal-to-noise ratio of the auditory signal by convolving the acoustic input with brown noise (random noise with greater energy at lower frequencies).

First, we present standard behavioral analyses of reaction time (RT) and accuracy of listeners' first gaze shifts after target noun onset. Then, we present two model-based analyses that link observable behavior to underlying psychological constructs. We use an exponentially weighted moving average (EWMA) method [@vandekerckhove2007fitting] to classify participants' gaze shifts as language-driven or random. In contrast to the standard RT/accuracy analysis, the EMWA approach allows us to quantify participants' willingness to generate gaze shifts after noun onset but before collecting sufficient information to seek the named referent. Higher values indicate that participants were waiting to shift until they had accumulated enough of the linguistic signal to locate the named referent. Finally, we use drift-diffusion models (DDMs) [@ratcliff2015individual] to ask whether behavioral differences in accuracy and RT are driven by a more cautious responding strategy or by more efficient information processing.

We predicted that processing speech in a noisy context would make participants less likely to shift before collecting sufficient information. This delay, in turn, would lead to a lower proportion of shifts flagged as random/exploratory in the EWMA analysis, and a pattern of DDM results indicating a prioritization of accuracy over and above speed of responding (see the Analysis Plan section below for more details on the models). We also predicted a developmental difference -- that children would produce a higher proportion of random shifts and accumulate information less efficiently compared to adults;  and a developmental parallel -- that children would show the same pattern of adapting gaze patterns to gather more visual information in the noisy processing context.

## Method

### Participants

```{r e1 filter}
d_e1 <- d %>% 
  filter(experiment != "kids_gaze",
         keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         gaze_condition == "straight_ahead")
```

```{r noise participants}
e1_adults <- d_e1 %>% 
  filter(age_category == "adults") %>% 
  select(subid, gender) %>% 
  unique() %>%
  group_by(gender) %>% 
  tally()

e1_kids <- d_e1 %>% 
  filter(age_category == "children") %>% 
  select(subid, age, gender) %>% 
  unique() %>% 
  group_by(gender) %>% 
  tally()

n_adults_run <- d %>% 
  filter(age_category == "adults") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_run <- d %>% 
  filter(age_category == "children", experiment == "kids_noise") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_filt <- n_kids_run - sum(e1_kids$n)
n_adults_filt <- n_adults_run - sum(e1_adults$n)
```

```{r}
ms_age <- d %>% 
  filter(age_category == "children") %>% 
  summarise(m = mean(age, na.rm = T) / 365,
            sd_age = sd(age, na.rm = T) / 365) %>% 
  mutate_all(round, 2)
```

Participants were native, monolingual English-learning children ($n=$ `r sum(e1_kids$n)`; `r e1_kids$n[1]` F, $M_{age}$ = `r ms_age$m` years, $SD_{age}$ = `r ms_age$sd_age`) and adults ($n=$ `r sum(e1_adults$n)`; `r e1_adults$n[1]` F). All participants had no reported history of developmental or language delay and normal vision/hearing. `r n_kids_filt + n_adults_filt` participants (`r n_kids_filt` children, `r n_adults_filt` adults) were run but not included in the analysis because either the eye tracker falied to calibrate (2 children, 3 adults) or the participant did not complete the task (9 children). 

```{r noise_acc_rt_e1_plot, include = T, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.6, out.width= "75%", fig.align='center', fig.cap = "Behavioral results for first shift Reaction Time (RT) and Accuracy. Panel A shows violin plots representing the distribution of RTs for each participant in each condition. Each point represents a participant's average RT. Color represents the processing context. The grey insets show the full posterior distribution of RT differences across conditions with the vertical dashed line representing the null value of zero condition difference. The green shading represents estimates in the predicted direction and above the null value while the red shading represents estimates below the null. Panel B shows the same information but for first shift accuracy."}
png::readPNG(here::here("paper/cogsci2018/figs/e1_behav_results.png")) %>% 
  grid::grid.raster()
```

### Stimuli 

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 2)) 
```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where’s the (target word) -- see panel A of Fig.\ \ref{fig:stimuli_plot}. The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=2)` ms. 

*Noise manipulation*. To create the stimuli for the noise condition, we convolved each recording with Brown noise using the Audacity audio editor. The average signal-to-noise ratio ^[The ratio of signal power to the noise power, with values greater than 0 dB indicating more signal than noise.] in the noise condition was 2.87 dB compared to the clear condition, which was 35.05 dB. 

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distractor image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of the target picture was counterbalanced across trials.

```{r ewma_violin_plot, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.35", out.width = "75%", fig.align='center', fig.cap = "EWMA results for children and adults. Each point represents the proportion of shifts categorized as language-driven (as opposed to guessing) for a single participant. Color represents the processing context."}

d_ewma_noise %>% 
  mutate(age_code = ifelse(str_detect(age_code, "child"), "children", "adults"),
         gaze_condition = ifelse(str_detect(condition, "gaze"), "gaze", 
                                 "straight_ahead"),
         noise_condition = ifelse(str_detect(condition, "noise"), "noise", "clear")) %>%
  filter(gaze_condition == "straight_ahead", 
         rt <= 2) %>% 
  group_by(subid, noise_condition, age_code, guess) %>% 
  summarise(count = n()) %>% 
  mutate(prop.responding = round(count / sum(count), 2)) %>% 
  filter(guess == "response") %>%
  ggplot(aes(x = fct_rev(age_code), y = prop.responding, color = noise_condition)) +
  #geom_line(aes(group = subid), color = "grey", alpha = 0.5) +
  geom_jitter(alpha = 0.8, shape = 21, 
              position = position_jitterdodge(jitter.width = 0.1, 
                                              dodge.width = 1)) +
  geom_violin(draw_quantiles = 0.5, trim = T, width = 1, size = 0.8, 
              adjust = 1, fill = NA) + 
  guides(fill = F) +
  lims(y = c(0,1)) +
  ggthemes::scale_color_ptol() +
  scale_x_discrete(expand = c(0,0.8), drop = FALSE) +
  labs(x = NULL, y = "Prop. Language-Driven Shifts", color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        text = element_text(size = 10),
        legend.title = element_text(size = 9)) 
```

### Design and procedure

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 60 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared (see Fig.\ \ref{fig:stimuli_plot}). Next, they processed the target sentence -- which consisted of a carrier phrase, a target noun, and a question -- followed by two seconds without language to allow for a response. Child participants saw 32 trials (16 noise trials; 16 clear trials) with several filler trials interspersed to maintain interest. Adult participants saw 64 trials (32 noise; 32 clear). The noise manipulation was presented in a blocked design with the order of block counterbalanced across participants. Audio levels were kept at a constant level across participants.

## Analysis plan

First, we present behavioral analyses of First Shift Accuracy and Reaction Time (RT) ^[See https://osf.io/g8h9r/ for a pre-registration of the analysis plan.]. RT corresponds to the latency to shift away from the central stimulus to either picture measured from the onset of the target noun. Accuracy corresponds to whether participants' first gaze shift landed on the target or the distracter picture. However, it is important to point out that when we analyze differences in accuracy, we are not making claims about the overall amount of time spent looking at the target vs. the distractor image – a measure typically used in analyses of the Visual World Paradigm.

We used the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and a within-participants manipulation -- by including random intercepts for each participant and item, and a random slope for each item and noise condition. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI). The HDI provides a range of credible values given the data and model. Finally, to estimate age-related differences, we fit two types of models: (1) age group (adults vs. children) as a categorical predictor and (2) age (in days) as a continuous predictor within the child sample.

Next, we present the two model-based analyses -- the EWMA and DDM. The goal of these models is to move beyond a description of the data and map behavioral differences in eye movements to underlying psychological variables. The EWMA method models changes in random shifting behavior as a function of RT. For each RT, the model generates two values: a "control statistic" (CS, which captures the running average accuracy of first shifts) and an "upper control limit" (UCL, which captures the pre-defined limit of when accuracy would be categorized as above chance level). Here, the CS is an expectation of random shifting to either the target or the distracter image (nonlanguage-driven shifts), or a Bernoulli process with probability of success 0.5. As RTs get slower, we assume that participants have gathered more information and should become more accurate (language-driven), or a Bernoulli process with probability success > 0.5. Using this model, we can quantify the proportion of gaze shifts that were language-driven as opposed to random responding. 

Following @vandekerckhove2007fitting, we selected shifts categorized as language-driven by the EWMA and fit a hierarchical Bayesian drift-diffusion model (HDDM). The DDM quantifies differences in the underlying decision process that lead to different patterns of behavior. The model assumes that people accumulate noisy evidence in favor of one alternative with a response generated when the evidence crosses a pre-defined decision threshold. Here, we focus on two parameters of interest: *boundary separation*, which indexes the amount of evidence gathered before generating a response (higher values suggest more cautious responding) and *drift rate*, which indexes the amount of evidence accumulated per unit time (higher values suggest more efficient processing). 

```{r hddm_plot_noise, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.35", out.width = "75%", fig.align='center', fig.cap = "HDDM results. Each panel shows the posterior distribution for either the boundary separation or drift rate parameters for children (top panels) and adults (bottom panels)."}

d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("gaze_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         age_code = factor(age_code, levels = c("children", "adults"))) %>% 
  ggplot(aes(x = param_value, color = condition)) +
  geom_line(stat = "density", size = 1, position = position_dodge(width=0.1)) +
  ggthemes::scale_color_ptol() +
  facet_grid(age_code~param_name, scales = "free") +
  labs(x = "Parameter Estimate", y = NULL, color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        axis.ticks = element_blank(), 
        axis.text.y = element_blank(),
        text = element_text(size = 10),
        legend.title = element_text(size =9)) 
```

## Results and Discussion

```{r noise filter}
d_e1_analysis <- d_e1 %>% 
  filter(rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center") %>% 
  mutate(shift_acc_num = ifelse(shift_accuracy_clean == "correct", 1, 0),
         log_rt = log(rt))
```

```{r noise summarize model output}
ms_acc_e1 <- d_models$acc_noise %>% 
  group_by(noise_condition, age_category) %>% 
  summarise(prop = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

ms_rt_e1 <- d_models$rt_noise %>% 
  group_by(noise_condition, age_category) %>% 
  mutate(rt_ms_scale = rt_ms_scale * 1000) %>% 
  summarise(m_rt = median(rt_ms_scale),
            hdi_lower = quantile(rt_ms_scale, probs = 0.025),
            hdi_upper = quantile(rt_ms_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 
```

```{r noise create contrasts}
noise_contrast <- d_models$acc_noise %>% 
  select(sample_id:acc_prob_scale, -param_est) %>% 
  spread(noise_condition, acc_prob_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)

noise_contrast_rt <- d_models$rt_noise %>% 
  select(sample_id:rt_ms_scale, -param_est) %>% 
  spread(noise_condition, rt_ms_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)
```

### Behavioral analyses:

```{r ms contrast noise rt}
ms_noise_con_rt <- noise_contrast_rt %>%
  mutate(noise_contrast = noise_contrast * 1000) %>% 
  summarise(m_rt = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r ms noise age continuous rt}
ms_noise_age_rt <- d_models$rt_noise_age %>% 
  mutate(age_beta = age_beta * 1000) %>% 
  summarise(m_rt = mean(age_beta),
            hdi_lower = quantile(age_beta, probs = 0.025),
            hdi_upper = quantile(age_beta, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

**RT.** To make RTs more suitable for modeling on a linear scale, we analyzed responses in log space with the final model specified as: \texttt{$log(RT) \sim noise\_condition + age\_group + (noise\_condition \mid sub\_id ) + (noise\_condition \mid target\_item)$}. Panel A of Fig.\ \ref{fig:noise_acc_rt_e1_plot} shows the full RT data distribution and the full posterior distribution of the estimated RT difference between the noise and clear conditions. Both children and adults were slower to identify the target in the noise condition (Children $M_{noise}$ = `r ms_rt_e1$m_rt[4]` ms; Adult $M_{noise}$ = `r ms_rt_e1$m_rt[3]` ms), as compared to the clear condition (Children $M_{clear}$ = `r ms_rt_e1$m_rt[2]` ms; Adult $M_{clear}$ = `r ms_rt_e1$m_rt[1]` ms). RTs in the noise condition were `r ms_noise_con_rt$m_rt[1]` ms slower on average, with a 95% HDI ranging from `r ms_noise_con_rt$hdi_lower[1]` ms to `r ms_noise_con_rt$hdi_upper[1]` ms, and not including the null value of zero condition difference. Older children responded faster than younger children ($M_{age}$ = `r ms_noise_age_rt$m_rt`, [`r ms_noise_age_rt$hdi_lower`, `r ms_noise_age_rt$hdi_upper`]), with little evidence for an interaction between age and condition. 

```{r ms contrast noise acc}
ms_noise_con_acc <- noise_contrast %>%
  summarise(m_acc = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

## hypothesis test
prob_diff0 <- noise_contrast %>% 
  summarise(prob = mean(noise_contrast >= 0)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

**Accuracy.** Next, we modeled adults and children's first shift accuracy using a mixed-effects logistic regression with the same specifications (see Panel B of Fig.\ \ref{fig:noise_acc_rt_e1_plot}). Both groups were more accurate than a model of random responding (null value of $0.5$ falling well outside the lower bound of the 95% HDI for all group means). Adults were more accurate ($M_{adults} =$ `r ms_acc_e1$prop[1] * 100`%) than children ($M_{children} =$ `r ms_acc_e1$prop[2] * 100`%). The key result is that both groups showed evidence of higher accuracy in the noise condition: children ($M_{noise}$ = `r ms_acc_e1$prop[4]* 100`%; $M_{clear}$ = `r ms_acc_e1$prop[2]* 100`%) and adults ($M_{noise}$ = `r ms_acc_e1$prop[3]* 100`%; $M_{clear}$ = `r ms_acc_e1$prop[1]* 100`%). Accuracy in the noise condition was on average `r ms_noise_con_acc$m_acc[1] * 100`%  higher, with a 95% HDI from `r ms_noise_con_acc$hdi_lower[1]* 100`% to `r ms_noise_con_acc$hdi_upper[1] * 100`%. Note that the null value of zero difference falls at the very edge of the HDI. But  `r prob_diff0$prob[1] * 100`% of the credible values are greater than zero, providing evidence for higher accuracy in the noise condition. Within the child sample, there was no evidence of a main effect of age or an interaction between age and noise condition.

### Model-based analyses:

```{r noise ewma group means summary}
# summarise group means for cutoffs
# ms_cuts_noise <- d_models$ewma_cuts_noise %>% 
#   group_by(age_category, noise_condition) %>% 
#   summarise(MAP = mean(param_est),
#             hdi_lower = quantile(param_est, probs = 0.025),
#             hdi_upper = quantile(param_est, probs = 0.975)) %>% 
#   mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
#   mutate(model = "EWMA", parameter = "cut point") %>% 
#   select(model, parameter, everything())

# summarise group means for prop guessing parameter
ms_guess_noise <- d_models$ewma_guess_noise %>% 
  group_by(age_category, noise_condition) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate(model = "EWMA", parameter = "prob. guessing") %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  select(model, parameter, everything())

ms_guess_noise_age <- d_models$ewma_guess_noise %>% 
  group_by(age_category) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate(model = "EWMA", parameter = "prob. guessing") %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  select(model, parameter, everything())

ms_cond_diff_guessing_m <- d_models$ewma_guess_noise  %>% 
  select(sample_id, noise_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`adults-children` = age_beta, 
         `noise-clear` = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

```{r ewma cond difference summary, eval = F, include = F}
## cut point model noise
ms_cond_diff_cuts <- d_models$ewma_cuts_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  rename(`age group` = age_beta, noise = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing <- d_models$ewma_guess_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`adults-children` = age_beta, 
         `noise-clear` = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## cut point model gaze
ms_cond_diff_cuts_gaze <- d_models$ewma_cuts_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  rename(`age group` = age_beta, gaze = straightahead_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing_gaze <- d_models$ewma_guess_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`gaze-straight_ahead` = straightahead_beta, 
         `adults-children` = age_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)
```

```{r ewma results table, results="asis", include = F, eval = F}
ewma_table <- bind_rows(ms_cond_diff_cuts, ms_cond_diff_guessing) %>% 
  select(-Exp.)

ewma_table$Parameter[2] <- ""
ewma_table$Parameter[4] <- ""

ewma_table <- xtable::xtable(ewma_table, 
                             caption = "EWMA results. The guessing parameter refers to the proportion of gaze shifts classified as random vs. language-driven with higher values indicating more random responding. Estimate refers to the difference between condition or age group.",
                             align = "lllr")

xtable::print.xtable(ewma_table, type="latex", comment = F, table.placement = "t",
      floating.environment = "table", 
      include.rownames=FALSE)
```

```{r ewma age continuous}
ms_ewma_age_kids <- d_models$ewma_guess_noise_age %>% 
  mutate(age_beta = age_beta * 1000) %>% 
  summarise(MAP = mean(age_beta),
            hdi_lower = quantile(age_beta, probs = 0.025),
            hdi_upper = quantile(age_beta, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

**EWMA.** Fig.\ \ref{fig:ewma_violin_plot} shows the proportion of shifts that the model classified as random vs. language-driven for each age group and processing context. On average, `r ms_guess_noise_age$MAP[2] * 100`% (95% HDI: `r ms_guess_noise_age$hdi_lower[2] * 100`%, `r ms_guess_noise_age$hdi_upper[2] * 100`%) of children's shifts were categorized as language-driven, which was significantly fewer than adults, `r ms_guess_noise_age$MAP[1] * 100`% (95% HDI: `r ms_guess_noise_age$hdi_lower[1] * 100`%, `r ms_guess_noise_age$hdi_upper[1] * 100`%). Critically, processing speech in a noisy context caused both adults and children to generate a higher proportion of language-driven shifts (i.e., fewer random, exploratory shifts away from the speaker), with the 95% HDI excluding the null value of zero condition difference ($\beta_{noise}$ = `r ms_cond_diff_guessing_m$MAP[2] * 100`%, [`r ms_cond_diff_guessing_m$hdi_lower[2] * 100`%, `r ms_cond_diff_guessing_m$hdi_upper[2] * 100`%]). Within the child sample, older children generated fewer random, early shifts ($M_{age}$ = `r ms_ewma_age_kids$MAP`, [`r ms_ewma_age_kids$hdi_lower`, `r ms_ewma_age_kids$hdi_upper`]). There was no eivdence of an interaction between age and condition. This pattern of results suggests that the noise condition caused participants to increase visual fixations to the language source, leading them to generate fewer exploratory, random shifts before accumulating sufficient information to respond accurately.

```{r hddm results}
hddm_table_age <- d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("straight_ahead_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(condition = ifelse(str_detect(condition, "noise"), "noise", "clear")) %>% 
  group_by(param_name, age_code) %>% 
  summarise(MAP = mean(param_value),
            HDI_lower = quantile(param_value, probs = 0.025),
            HDI_upper = quantile(param_value, probs = 0.975)) %>% 
  mutate_at(vars(MAP, HDI_lower, HDI_upper), round, digits = 2)


hddm_table_contrast <- d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("straight_ahead_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(condition = ifelse(str_detect(condition, "noise"), "noise", "clear")) %>% 
  dplyr::filter(param_name == "boundary", age_code == "children") %>% 
  group_by(condition) %>% 
  mutate(sample_id = 1:n()) %>% 
  select(param_value, condition, sample_id) %>% 
  tidyr::spread(condition, param_value) %>%
  mutate(cond_diff = noise - clear) %>% 
  summarise(MAP = mean(cond_diff),
            HDI_lower = quantile(cond_diff, probs = 0.025),
            HDI_upper = quantile(cond_diff, probs = 0.975)) %>% 
  mutate_at(vars(MAP, HDI_lower, HDI_upper), round, digits = 2)
```

```{r print hddm table, include = F, results = "asis", eval = F}
hddm_table <- d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("straight_ahead_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(Condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         `Age Group` = ifelse(str_detect(age_code, "kid"), "children", "adults")) %>% 
  group_by(param_name, Condition, `Age Group`) %>% 
  summarise(MAP = mean(param_value),
            HDI_lower = quantile(param_value, probs = 0.025),
            HDI_upper = quantile(param_value, probs = 0.975)) %>% 
  rename(Parameter = param_name) %>% 
  mutate_at(vars(MAP, HDI_lower, HDI_upper), round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(`Estimate [95% HDI]` = paste0(MAP, " [", HDI_lower, ", ", HDI_upper, "]")) %>% 
  select(-MAP, -HDI_lower, -HDI_upper)

# remove double entries in the table
hddm_table$Parameter[1] <- ""
hddm_table$Parameter[3] <- ""
hddm_table$Parameter[4] <- ""

hddm_table$Parameter[5] <- ""
hddm_table$Parameter[7] <- ""
hddm_table$Parameter[8] <- ""

# conditions
hddm_table$Condition[2] <- ""
hddm_table$Condition[4] <- ""
hddm_table$Condition[6] <- ""
hddm_table$Condition[8] <- ""

# now make the xtable
hddm_table <- xtable::xtable(hddm_table, 
                             caption = "HDDM parameter estimates for each age group and noise condition. The drift rate parameter indexes processing efficiency and the boundary separation parameter indexes participants' information accumulation threshold.",
                             align = "llllr")

hlines <- c(-1, 0, 4, nrow(hddm_table))

xtable::print.xtable(hddm_table, type="latex", comment = F, table.placement = "t",
      hline.after = hlines,
      floating.environment = "table", 
      include.rownames=FALSE)
```

**HDDM.** Fig.\ \ref{fig:hddm_plot_noise} shows the full posterior distributions for the HDDM output. Children had lower drift rates (children $M_{drift}$ = `r hddm_table_age$MAP[4]`; adults $M_{drift}$ = `r hddm_table_age$MAP[3]`) and boundary separation estimates (children $M_{boundary}$ = `r hddm_table_age$MAP[2]`; adults $M_{boundary}$ = `r hddm_table_age$MAP[1]`) as compared to adults, suggesting that children were less efficient and less cautious in their responding. The noise manipulation selectively affected the boundary separation parameter, with higher estimates in the noise condition for both age groups ($\beta_{noise}$ = `r hddm_table_contrast$MAP[1]`, [`r hddm_table_contrast$HDI_lower[1]`, `r hddm_table_contrast$HDI_upper[1]`]). This result suggests that participants' in the noise condition prioritized information accumulation over speed when generating an eye movement in response to the incoming language. This increased decision threshold led to higher accuracy. Moreover, the high overlap in estimates of drift rate suggests that participants were able to integrate the visual and auditory signals such that they could achieve a level of processing efficiency comparable to the clear processing context.

Taken together, the behavioral and EWMA/HDDM results provide converging support for the predictions of our information-seeking account. Processing speech in noise caused listeners to seek additional visual information to support language comprehension. Moreover, we observed a very similar pattern of behavior in children and adults, with both groups producing more language-driven shifts and prioritizing accuracy over speed in the more challenging, noisy environment. 

# General Discussion

Language comprehension in grounded contexts involves integrating information from the visual and linguistic signals. But the value of integrating visual information depends on the processing context. Here, we presented a test of an information-seeking account of eye movements during language processing: that listeners flexibly adapt gaze patterns in response to the value of seeking visual information for accurate language understanding. We showed that children and adults generate slower but more accurate gaze shifts away from a speaker when processing speech in a noisy context. Both groups showed evidence of prioritizing information accumulation over speed (HDDM) while guessing less often (EWMA). Listeners were able to achieve higher accuracy in the more challenging, noisy context. Together, these results suggest that in settings with a degraded linguistic signal, listeners support language comprehension by seeking additional language-relevant information from the visual world.

These results synthesize ideas from several research programs, including work on language-mediated visual attention [@tanenhaus1995integration], goal-based accounts of vision during everyday tasks [@hayhoe2005eye], and work on effortful listening [@van2014listening].  Moreover, our findings parallel recent work by @mcmurray2017waiting showing that individuals with Cochlear Implants, who are consistently processing degraded auditory input, are more likely to delay the process of lexical access as measured by slower gaze shifts to named referents and fewer incorrect gaze shifts to phonological onset competitors. @mcmurray2017waiting also found that they could replicate these changes to gaze patterns in adults with typical hearing by degrading the auditory stimuli so that it shared features with the output of a cochlear implant (noise-vocoded speech).

The results reported here also dovetail with recent developmental work by @yurovsky2017preschoolers. In that study, preschoolers, like adults, were able to integrate top-down expectations about the kinds of things speakers are likely to talk about with bottom-up cues from auditory perception. @yurovsky2017preschoolers situated this finding within the framework of modeling language as a *noisy channel* where listeners combine expectations with perceptual data and weight each based on its reliability. Here, we found a similar developmental parallel in language processing: that 3-5 year-olds, like adults, adapted their gaze patterns to seek additional visual information when the auditory signal became less reliable. This adaptation allowed listeners to generate more accurate responses in the more challenging, noisy context.

This work has several important limitations that pave the way for future work. First, we chose to focus on a single decision about visual fixation to provide a window onto the dynamics of decision-making across different language processing contexts. But our analysis does not consider the rich information present in the gaze patterns that occur leading up to this decision. In our future work, we aim to measure how changes in the language environment might lead to shifts in the dynamics of gaze across a wider timescale. For example, perhaps listeners gather more information about the objects in the scene before the sentence in anticipation of allocating more attention to the speaker once they start to speak. Second, we chose one instantiation of a noisy processing context -- random background noise. But we think our findings should generalize to contexts where other kinds of noise -- e.g., uncertainty over a speaker's reliability or when processing accented speech -- make gathering visual information from the speaker more useful for language understanding.

This experiment tested the generalizability of our information-seeking account of eye movements within the domain of grounded language comprehension. But the account could be applied to the language acquisition context. Consider that early in language learning, children are acquiring novel word-object links while also learning about visual object categories. Both of these tasks produce different goals that should, in turn, modulate children's decisions about where to allocate visual attention -- e.g., seeking nonlinguistic cues to reference such as eye gaze and pointing become critical when you are unfamiliar with the information in the linguistic signal. More generally, this work integrates goal-based models of eye-movements with language comprehension in grounded, social contexts. This approach presents a way forward for explaining fixation behaviors across a wider variety processing contexts and during different stages of language learning.

```{r session info, include = F}
sessionInfo() %>% pander::pander(compact = F)
```

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering All data and code for this paper are available at\\\url{https://github.com/kemacdonald/speed-acc/tree/master/paper/cogsci2018}}} \vspace{1em}

# Acknowledgements

We are grateful to the families who participated in this research. Thanks to Tami Alade and Hannah Slater for help with data collection. And thanks to Bria Long for helpful feedback on this paper. This work was supported by an NSF GRFP to KM and a Jacobs Foundation Fellowship to MCF.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
