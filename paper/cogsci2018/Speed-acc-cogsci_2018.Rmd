---
title: "Adults and preschoolers seek visual information to support language comprehension in noisy environments"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
  \author{ {\large \bf Kyle MacDonald}$^1$ (kylem4@stanford.edu), {\large \bf Virginia Marchman}$^1$ (marchman@stanford.edu), 
  \\ {\large \bf Anne Fernald}$^1$ (afernald@stanford.edu), {\large \bf Michael C. Frank}$^1$ (mcfrank@stanford.edu) 
    \\ $^1$ Department of Psychology Stanford University}

abstract: 
    "Language comprehension is a multisensory and interactive process. When interpreting an utterance, listeners rapidly integrate information from both the visual and the linguistic signals. But the quality of each information source varies depending on the processing context, e.g., understanding speech in noise. Here, we present a strong test of the hypothesis that listeners will adapt the dynamics of gaze during lexical access to seek higher value visual information that supports comprehension. We measured the timing and accuracy of adults (n=31) and children's (n=40, 3-5 y.o.) eye movements during a real-time language comprehension task and found that both age groups delayed the timing of gaze shifts away from a speaker's face when processing speech in noise. Interestingly, this delay resulted in higher information accumulation from the visual signal, more accurate shifts, and fewer random eye movements to the rest of the visual world. This results suggest that the dynamics of gaze adapt to the processing demands of different contexts and even young listeners will seek visual information that supports real-time language comprehension."
    
keywords:
  "eye movements; language processing; information-seeking; speech in noise;"

output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/',echo=F, warning=F, cache=T, message=F, include =F,
                      sanitize = T)

library(here); library(kableExtra)
source(here::here("R/helper_functions/libraries_and_functions.R"))
source(here::here("R/helper_functions/ewma_helper_funs.R"))
```

```{r data paths}
data_path <- "data/3_final_merged_data/first_shifts/"
ewma_path <- here::here("data/3_final_merged_data/ewma_output/")
hddm_path <- "data/3_final_merged_data/hddm_output/"
```

```{r read data}
d_kids_noise <- read_csv(here::here(data_path, "speed_acc_child_noise_fstshift_tidy.csv")) 
d_kids_gaze <- read_csv(here::here(data_path, "speed_acc_child_gaze_fstshift_tidy.csv")) 
d_adults <- read_csv(here::here(data_path, "speed_acc_adult_ng_fstshift_tidy.csv")) 
```

```{r read ewma output}
noise_ewma_files <- c("speed_acc_kids_noise_ewma_results.csv",
                      "speed_acc_adult_ng_ewma_results.csv")

d_ewma_noise <- noise_ewma_files %>% 
  purrr::map_df(read_ewma, path = ewma_path) 
```

```{r read hddm output}
d_hddm <- read_csv(here::here(hddm_path, "hddm_tidy.csv"))
```

```{r read bda output}
d_models <- readRDS(here::here("data/3_final_merged_data/first_shifts", "speed-acc-posterior-samples.rds"))
```

```{r clean datasets for merge}
d_adults %<>% select(-age)

d_kids_noise %<>% mutate(gaze_condition = ifelse(gaze_condition == "no_gaze", 
                                                 "straight_ahead", 
                                                 gaze_condition))

d_kids_gaze %<>% mutate(noise_condition = ifelse(noise_condition == "no_noise", 
                                                 "clear", 
                                                 noise_condition))
```

```{r merge datasets}
d <- bind_rows(mutate(d_kids_noise, experiment = "kids_noise", age_category = "children"),
               mutate(d_kids_gaze, experiment = "kids_gaze", age_category = "children"),
               mutate(d_adults, experiment = "adults_ng", age_category = "adults")) %>% 
  select(-resp_onset_type_fact, -subid_short) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

# uncomment below to test that we have the right number of rows after the merge (result should be TRUE)
# nrow(d_kids_gaze) + nrow(d_kids_noise) + nrow(d_adults) == nrow(d)
```

```{r read stimuli information}
d_gaze_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-gaze-trial-info.csv"),
                        col_types = cols(.default = "c"))
d_noise_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-child-noise-trial-info.csv"),
                         col_types = cols(.default = "c"))
d_adult_ng_stim <- read_csv(here::here("data/0b_trial_information/speed-acc-adult-ng-trial-info.csv"),
                            col_types = cols(.default = "c"))
```

```{r merge stim info}
d_stim <- bind_rows(mutate(d_noise_stim, experiment = "kids_noise"),
                    mutate(d_gaze_stim, experiment = "kids_gaze"),
                    mutate(d_adult_ng_stim, experiment = "adults_ng"))
```

# Introduction

Real-time language comprehension is an interactive and multimodal phenomenon. As skilled listeners, we continually integrate information from both the visual and the linguistic signal in order to understand what others are saying. A classic demonstration of this integration process is the "McGurk effect" where a speaker's mouth movements suggest one sound while their acoustic output suggests another. This conflict results in the listener perceiving a third, intermediate sound [@macdonald1978visual]. Findings such as these have inspired prominent theories of speech perception [@mcclelland2006there] and lexical processing [@macdonald2006constraint; @smith2017multimodal] that argue for the fundamental role of *interactive* processes -- where listeners process information from multiple sources in parallel. Moreover, empirical work on speech perception shows that adults are better able to "recover" linguistic information in noisy contexts when they have visual access to a speaker's face [@erber1969interaction]

However, the usefulness of integrating visual information varies depending on features of the listener and features of the procssing context. Consider the case of processing a visual-manual language like American Sign Language (ASL). Here, the value of allocating visual fixations to the language source (i.e., the signer) is incredibly high since all of the language-relevant information is available in that location. In our prior work, we showed that, compared to spoken language learners, ASL-learners were more sensitive to the higher value of eye movements in a visual language, choosing to prioritize information accumulation and accuracy over and above speed when deciding to shift gaze away from another signer [@macdonald2017info]. To explain this difference, we proposed an information-seeking account inspired by goal-based theories of vision [@hayhoe2005eye]: that signers adapted the dynamics of gaze during lexical access to seek information that supported accurate language comprehension. 

In the work reported here, we test a specific prediction of this information-seeking account and ask whether listeners would adapt patterns of fixation when looks to a speaker become more valuable -- as is the case of processing speech in noise. Consider the familair example of a speaker who asks you to "Pass the salt" in a noisy restaurant where it is difficult to understand the request. The intuition is that comprehension can be facilitated by gathering information via fixations to the speaker by reading lips or perhaps the direction of gaze.

There has been recent interest in understanding how gaze patterns during lexical access might adapt to different contexts that deviate from the well-controlled lab settings that are typically used in psycholinguistic tasks. For example, recent work by @mcmurray2017waiting shows that individuals with Cochlear Implants, who are consistenly processing degraded auditory input, are more likely to delay the process of lexical access as compared to listeners with typical hearing. Our work asks a related question about whether the interaction between language and visual attention interaction flexibly adapts to the features of the processing context. 

Specifically, we hypothesized that a noisy auditory environment would make the auditory signal less reliable, and in turn increase the value of fixating on a speaker for the task of language understanding. Our key behavioral prediction is that listeners who process speech in noise will delay generating an eye movement away from a speaker until they have accumulated additional information about the named referent. In other words, we predict that noisy contexts will lead to more fixations allocated to a language-relevant aspect of the visual world. The next section outlines the specific measurements and models that we use to quantify the evidence for this hypothesis.

<!-- The study of eye movements during language comprehension has provided fundamental insights into the interaction between conceptual representations of the world and the incoming linguistic signal. For example, research shows that adults and children will rapidly shift visual attention upon hearing the name of an object in the visual scene, with a high proportion of shifts occurring prior to the offset of the word [@allopenna1998tracking; @tanenhaus1995integration]. Moreover, researchers have found that conceptual representations activated by fixations to the visual world can modulate subsequent eye movements during language processing [@altmann2007real].  -->

<!-- The majority of this work has used eye movements as a measure of the output of the underlying language comprehension process, often using linguistic stimuli that come from a disembodied voice. But in real world contexts, people also gather information about the linguistic signal by fixating on the language source. Consider a speaker who asks you to "Pass the salt" but you are in a noisy room, making it difficult to understand the request. Here, comprehension can be facilitated by gathering information via (a) fixations to the nonlinguistic visual world (i.e., encoding the objects that are present in the scene) or (b) fixations to the speaker (i.e., reading lips or perhaps the direction of gaze).  -->

<!-- But, this situation creates a tradeoff where the listener must decide what kind of information to gather and at what time. How do we decide where to look? We propose that people modulate their eye movements during language comprehension in response to tradeoffs in the value of gathering different kinds of information. We test this adaptive tradeoff account using two case studies that manipulate the value of different fixation locations for language understanding: a) a comparison of processing sign vs. spoken language in children (E1), and b) a comparison of processing printed text vs. spoken language in adults (E2). Our key prediction is that competition for visual attention will make gaze shifts away from the language source less valuable than fixating the source of the linguistic signal, leading people to generate fewer exploratory, nonlanguage-driven eye movements. -->

<!---
Using ideas from the field of natural vision where eye movements are modeled as a way to reduce uncertainty about the world and to maximize the expected reward of future actions [@hayhoe2005eye]

We propose that people modulate their eye movement behavior in response to changes in the value of gathering different kinds of information. We test this information-seeking account using two case studies that manipulate the value of different fixation locations in the visual world for language understanding: a) a comparison of processing a visual-manual vs. a spoken language in children (Experiment 1), and b) a comparison of processing printed text vs. spoken language in adults (Experiment 2).

For example, imagine there is only one red object amongst many others, and you hear someone say, "Pass me the red _____!" If you have successfully encoded the visual world, then the adjective "red" allows you to constrain the speaker's intended meaning and respond rapidly and accurately. 

In contrast, researchers in the fields of natural vision have modeled fixations to the visual world as a tool for information gathering [@hayhoe2005eye]. In this approach, eye movements reflect a goal to gather information to reduce uncertainty and to maximize the expected reward of future actions. For example, @hayhoe2005eye review evidence that people do not fixate on the most salient aspects of a visual scene, but instead focus on aspects that are most helpful for the current task such as choosing to fixate on an upcoming obstacle when walking.

of information-seeking framework to account for a wider variety of fixation patterns during language comprehension. We characterize eye movements as a tradeoff between gathering information about the nonlinguistic visual world and monitoring the incoming linguistic signal.

To test the predictions of our account, we present two case studies where information about the linguistic signal can be gathered via fixations to the language source: processing American Sign Language (ASL) and processing displays of printed text. 

We assume that the goal is to maximize the chance of making a correct future response (in the context of our task, resolving reference rapidly and accurately by looking at the object that is being talked about).
-->

\label{}

```{r stimuli_plot, include = T, fig.env = "figure*", fig.pos = "tb", fig.width=6, fig.asp = 0.4, out.width= "80%", fig.align='center', fig.cap = "Stimuli for E1 and E2. Panel A shows the layout of the three fixation locations (speaker, target, and distracter), and the timecourse of a single trial. Panel B shows a visual representation of the clear and noisy waveforms used in E1. Panel C shows the gaze manipulation used in E2."}
grid::grid.raster(png::readPNG(here::here("paper/cogsci2018/figs/stimuli_info.png")))
```

# Experiment

In this experiment we test whether our information-seeking account of eye movements would generalize to a novel and ecologically valid processing context -- speech in noise. We recorded eye movements during a real-time language comprehension task where children and adults processed familiar sentences (e.g., "Where's the ball?") while looking at a simplified visual world with three fixation targets (see Fig 1). Using a within-participants design, we manipulated the signal-to-noise ratio of the auditory signal by convolving the language with brown noise. We predicted that processing speech in noise would increase the value of fixating on the speaker, causing listeners to gather additional information before generating a shift to the named referent even after the target word began unfolding in time. 

To quantify evidence for our prediction, we compare the Accuracy and Reaction Times (RTs) of participants' first shifts after target noun onset between the noisy and clear contexts. We take a "micro-level" approach and use first shifts as window onto changes in the underlying decision processes that generate eye movements. However, it is important to point out that when we analyze differences in Accuracy, we are not making claims about overall amount of time spent looking at the target vs. the distractor image -- a measure that is typically used in analyses of the Visual World Paradigm.

We also present two model-based analyses that link the observable behavior to underlying psychological constructs of interest. First, we use an exponentially weighted moving average (EWMA) method [@vandekerckhove2007fitting] to classify participants' gaze shifts as language-driven or random. In contrast to the standard RT/Accuracy analysis, the EMWA approach allows us to quantify participants' willingness to generate gaze shifts after noun onset but before collecting sufficient information to seek the named referent. Higher values indicate that participants were shifting early and equally likely to land on the target or distractor image.

Next, we use drift-diffusion models (DDMs) [@ratcliff2015individual] to ask whether behavioral differences in Accuracy and RT are driven by a more cautious responding strategy or by more efficient information processing -- an important distinction for our theoretical account. We predicted that processing speech in noise would make participants less likely to shift before collecting sufficient information, leading to a lower proportion of shifts flagged as random in the EWMA, and a higher boundary separation estimate in the DDM, indicating a prioritization of accuracy over and above speed. 

<!---
Since our results are complex, we preview them here: when the center stimulus was an Object or a Bullseye, children's first shifts away from the center stimulus were fast and at chance, and models suggested they never stopped guessing even when the language was informative. In contrast, for the Face condition and even more so for the signers, children fixated on the speaker to gather information and generated more accurate first shifts. In other words, their eye movements reflected a tradeoff between the value of gathering information from the speaker and exploring the nonlinguistic visual world.
-->

```{r noise_acc_rt_e1_plot, include = T, fig.env = "figure*", fig.pos = "t", fig.width=5, fig.asp = 0.7, out.width= "85%", fig.align='center', fig.cap = "Behavioral results for first shift Reaction Time (RT) and Accuracy. Panel A shows violin plots representing the distribution of RTs for each participant in each condition. Each black point represents a participant. The dark red points represent the model estimate for the group mean with the error bars showing the 95\\% Highest Density Interval around that point estimate. The grey inset shows the full posterior distribution of the plausible RT differences across conditions with the vertical dashed line representing the null value of zero condition difference. The green shading represents estimates in the predicted direction and above the null value while the red shading represents estimates below the null. Panel B shows the same information but for Accuracy."}
png::readPNG(here::here("paper/cogsci2018/figs/e1_behav_results.png")) %>% 
  grid::grid.raster()
```

## Method

### Participants

```{r e1 filter}
d_e1 <- d %>% 
  filter(experiment != "kids_gaze",
         keep_runsheet %in% c("yes", "keep"), 
         keep_et == "include",
         gaze_condition == "straight_ahead")
```

```{r noise participants}
e1_adults <- d_e1 %>% 
  filter(age_category == "adults") %>% 
  select(subid, gender) %>% 
  unique() %>%
  group_by(gender) %>% 
  tally()

e1_kids <- d_e1 %>% 
  filter(age_category == "children") %>% 
  select(subid, age, gender) %>% 
  unique() %>% 
  group_by(gender) %>% 
  tally()

n_adults_run <- d %>% 
  filter(age_category == "adults") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_run <- d %>% 
  filter(age_category == "children", experiment == "kids_noise") %>% 
  select(subid) %>% 
  unique() %>%
  tally() %>% 
  pull(n)

n_kids_filt <- n_kids_run - sum(e1_kids$n)
n_adults_filt <- n_adults_run - sum(e1_adults$n)
```

Participants were native, monolingual English-learning children ($n=$ `r sum(e1_kids$n)`; `r e1_kids$n[1]` F, `r e1_kids$n[2]` M) and adults ($n=$  `r sum(e1_adults$n)`; `r e1_adults$n[1]` F, `r e1_adults$n[2]` M). All participants had no reported history of developmental or language delay and normal vision. `r n_kids_filt + n_adults_filt` participants (`r n_kids_filt` children, `r n_adults_filt` adults) were run but not included in the analysis because either the eye tracker falied to calibrate or the participant did not complete the task. 

### Stimuli 

```{r linguistic stimuli length}
d_word_lengths <- d_stim %>% 
  filter(!is.na(noun)) %>% 
  select(noun, carrier, noun_onset_sec:noun_offset_frames) %>% 
  unique() %>% 
  mutate_at(vars(starts_with("noun_")), as.numeric) %>% 
  mutate(
    length_frames =  ( (noun_offset_sec * 33) + noun_offset_frames ) - ( (noun_onset_sec * 33) + (noun_onset_frames) ),
    length_ms = length_frames * 33
  ) 

ms_words_length <- d_word_lengths %>% 
  group_by(carrier, noun) %>% 
  summarise(m_length = mean(length_ms)) %>% 
  group_by(noun) %>% 
  summarise(m = mean(m_length) %>% round(digits = 2)) 
```

*Linguistic stimuli.* The video/audio stimuli were recorded in a sound-proof room and featured two female speakers who used natural child-directed speech and said one of two phrases: "Hey! Can you find the (target word)" or "Look! Where’s the (target word) -- see panel A of Fig 1. The target words were: ball, bunny, boat, bottle, cookie, juice, chicken, and shoe. The target words varied in length (shortest = `r min(ms_words_length$m)` ms, longest = `r max(ms_words_length$m)` ms) with an average length of `r mean(ms_words_length$m) %>% round(digits=2)` ms. 

*Noise manipulation*. To create the stimuli in the noise condition, we convolved each recording with Brown noise using the Audacity audio editor. The average signal-to-noise ratio ^[The ratio of signal power to the noise power, with values greater than 0 dB indicating more signal than noise.] in the noise condition was 2.87 dB compared to the clear condition, which was 35.05 dB. 

*Visual stimuli.* The image set consisted of colorful digitized pictures of objects presented in fixed pairs with no phonological overlap between the target and the distractor image (cookie-bottle, boat-juice, bunny-chicken, shoe-ball). The side of the target picture was counterbalanced across trials.

### Design and procedure

<!---
Participants viewed the ASL task on a 27" monitor. Children sat on their caregiver’s lap, and the child’s gaze was recorded using a digital camcorder set up behind the monitor. On each trial, pictures of two familiar objects appeared on the screen, a target object corresponding to the target noun, and a distracter object matched for visual salience. Between the two pictures was a central video of an adult female signing the name of one of the pictures. Participants saw 32 test trials with five filler trials (e.g. “YOU LIKE PICTURES? MORE WANT?”) interspersed to maintain children’s interest.

Participants viewed the Face, Object, and Bullseye tasks on a large projector screen in a sound-treated testing booth. Similar to the ASL task, at the beginning of each test trial, pictures of two familiar objects appeared on the screen and then a center stimulus appeared between the two pictures. The center stimulus varied across the three tasks: Face, Object, and Bullseye (see Figure 1 for details). Participants saw approximately 32 test trials with several filler trials interspersed to maintain children’s interest.

*Trial structure.* On each trial, the child saw two images of familiar objects on the screen for two seconds before the center stimulus appeared. This time allowed the child to visually explore both images. Next, the target sentence -- which consisted of a carrier phrase, target noun, and question sign -- was presented, followed by two seconds without language to allow the child to respond to the signer's sentence. The trial structure of the Face, Object, and Bullseye tasks were highly similar: children were given two seconds to visually explore the objects prior to the appearance of the center stimulus, then processed a target sentence, and finally were given two seconds of silence to generate a response to the target noun.

*Coding.* Participants’ gaze patterns were videotaped and later coded frame-by-frame at 33-ms resolution by trained coders blind to target side.  On each trial, coders indicated whether the eyes were fixated on the central signer, one of the images, shifting between pictures, or away (off), yielding a high-resolution record of eye movements aligned with target noun onset. Prior to coding, all trials were pre-screened to exclude those few trials on which the participant was inattentive or there was external interference.
-->

Participants viewed the task on a screen while their gaze was tracked using an SMI RED corneal-reflection eye-tracker mounted on an LCD monitor, sampling at 60 Hz. The eye-tracker was first calibrated for each participant using a 6-point calibration. On each trial, participants saw two images of familiar objects on the screen for two seconds before the center stimulus appeared (see Fig 1). Next, they processed the target sentence -- which consisted of a carrier phrase, a target noun, and a question -- followed by two seconds without language to allow for a response. Child participants saw 32 trials (16 noise trials; 16 clear trials) with several filler trials interspersed to maintain interest. Adult participants saw 64 trials (32 noise; 32 clear). The noise manipulation was presented in a blocked design with the order of block counterbalanced across participants.

<!--
We excluded RTs longer than two seconds since these shifts are unlikely to be generated in response to the incoming language stimulus (see Ratcliff, 1993). 

This measure reflects the accuracy of language-driven saccades to the visual world. Mean first shift accuracy scores were computed for each participant for both correct and incorrect shifts. Trials where the participants did not generate a shift were not included in the computation.
-->

## Results and Discussion

### Analysis plan

<!--
First, we present behavioral analyses of accuracy and RT. Since RTs are not normally distributed, we log transformed all RTs for our statistical analyses. To quantify differences between groups, we used the `lme4` R package [@bates2013lme4] to fit mixed-effects regression models that included a by-subject random intercept to account for repeated measures from each participant. All data and analysis code can be found in the online repository for this project: https://github.com/kemacdonald/speed-acc.  

Next, we present two model-based analyses that quantify different patterns of eye movements across the four language comprehension tasks. First, we use an Exponentially weighted moving average (EWMA) method [@vandekerckhove2007fitting] to model the proportion of nonlanguage-driven shifts away from the center stimulus to the visual world. The goal of  the EWMA is to identify whether a process has deviated from a pre-defined "control state" by taking into account the prior behavior of the process, weighing recent observations more heavily. The model generates two values: a "control statistic" (CS) and an "upper control limit" (UCL) for each point in the RT distribution. Once the CS becomes larger than the UCL, the process is determined to have exited the control state. ^[$c_s = \lambda x_s + (1-\lambda)c_{s-1}$ where the $\lambda$ parameter determines the number of prior RTs that are included in the moving average computation. $UCL_s = c_0 + L\sigma_0\sqrt{\frac{\lambda}{2-\lambda}[1-(1-\lambda)^{2s}]}$ where $L$ controls the width of the control limits with higher values leading to a more conservative test. We chose values for these parameters based on prior work using the EWMA approach with 2-AFC speeded decision tasks [@vandekerckhove2007fitting]]

Here, we adapt the EWMA approach to model changes in the process that generate eye movements away from the center stimulus to the visual world. We define the control state as an expectation of nonlanguage-driven shifts and model this as a Bernoulli process with probability of success 0.5. As the sentence unfolds, we assume that participants gather more of the linguistic information prior to shifting and the underlying process should bias towards more accurate shifts or a Bernoulli process with probability success > 0.5. With this model, we can compare across our groups: a) the cutoff point when the CS exceeded the UCL indicating that participants started to generate language-driven shifts and b) the proportion of shifts that the model categorizes as language-driven vs. exploratory.

Finally, we use Drift-diffusion models (DDMs) [@ratcliff2015individual] to quantify differences in the dynamics of speed and accuracy for eye movements generated in response to the incoming linguistic signal. DDMs form a class of sequential decision-making models designed specifically for rapid two-alternative forced choice tasks. The model assumes that people accumulate noisy evidence in favor of one alternative with a response generated when the evidence crosses a pre-defined decision threshold. The DDM approach is useful because it can account for all components of the behavioral response: correct and incorrect RT distributions. Moreover, the parameters of the DDM map onto meaningful psychological variables of interest. Here we focus on two of these parameters: **boundary separation**, which maps onto the amount of evidence gathered before generating a response (higher values suggest a prioritization of accuracy over speed) and **drift rate**, which maps onto the amount of evidence that is accumulated per unit time (higher values indicate more efficient processing). 

We chose to implement a hierarchical Bayesian version of the DDM using the HDDM Python package [@wiecki2013hddm] because we were dealing with relatively few trials from child participants and recent simulation studies have shown that the HDDM approach was better than other DDM fitting methods when the number of observations was small [@ratcliff2015individual].  
-->

First, we present behavioral analyses of First Shift Accuracy and Reaction Time (RT) ^[See https://osf.io/g8h9r/ for a pre-registration of the analysis plan.]. RT corresponds to the latency to shift away from the central stimulus to either picture measured from the onset of the target noun. (all RTs were analyzed in log space). Accuracy corresponds to whether participantss first gaze shifta landed on the target or the distracter picture. We used the `rstanarm` [@gabry2016rstanarm] package to fit Bayesian mixed-effects regression models. The mixed-effects approach allowed us to model the nested structure of our data -- multiple trials for each participant and item, and a within-participants manipulation -- by including random intercepts for each participant and item, and a random slope for each item and noise condition. We used Bayesian estimation to quantify uncertainty in our point estimates, which we communicate using a 95% Highest Density Interval (HDI). The HDI provides a range of credible values given the data and model. All analysis code can be found in the online repository for this project: https://github.com/kemacdonald/speed-acc/R/analysis. 

Next, we present the two model-based analyses -- the EWMA and DDM. The goal of these models is to move beyond a description of the data and map behavioral differences in eye movements to underlying psychological variables. The EWMA method models changes in random shifting behavior as a function of RT. For each participant, the model classifies the proportion of shifts that were likely to be language-driven as opposed to random responding, which we call the *guessing* parameter. 

```{r ewma_violin_plot, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.4", out.width = "85%", fig.align='center', fig.cap = "EWMA results for children and adults. Each point represents the proportion of shifts categorized as guessing for a single participant. The color of the violin plot represents the processing context: noise vs. clear."}

d_ewma_noise %>% 
  mutate(age_code = ifelse(str_detect(age_code, "child"), "children", "adults"),
         gaze_condition = ifelse(str_detect(condition, "gaze"), "gaze", 
                                 "straight_ahead"),
         noise_condition = ifelse(str_detect(condition, "noise"), "noise", "clear")) %>%
  filter(gaze_condition == "straight_ahead", 
         rt <= 2) %>% 
  group_by(subid, noise_condition, age_code, guess) %>% 
  summarise(count = n()) %>% 
  mutate(prop.responding = round(count / sum(count), 2)) %>% 
  filter(guess == "response") %>%
  ggplot(aes(x = fct_rev(age_code), y = prop.responding, color = noise_condition)) +
  #geom_line(aes(group = subid), color = "grey", alpha = 0.5) +
  geom_jitter(alpha = 0.8, shape = 21, 
              position = position_jitterdodge(jitter.width = 0.1, 
                                              dodge.width = 1)) +
  geom_violin(draw_quantiles = 0.5, trim = T, width = 1, size = 0.8, 
              adjust = 1, fill = NA) + 
  guides(fill = F) +
  lims(y = c(0,1)) +
  ggthemes::scale_color_ptol() +
  scale_x_discrete(expand = c(0,0.8), drop = FALSE) +
  labs(x = NULL, y = "Prop. Language-Driven Shifts", color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        text = element_text(size = 10),
        legend.title = element_text(size = 9)) 
```

```{r hddm_plot_noise, include = T, fig.env = "figure", fig.pos = "t", fig.width=3, fig.asp = "0.4", out.width = "85%", fig.align='center', fig.cap = "HDDM results. Each panel shows the posterior distribution for either the boundary separation or drift rate parameters for children (top panels) and adults (bottom panels)."}

d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("gaze_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         age_code = ifelse(age_code == "kid", "children", "adults"),
         age_code = factor(age_code, levels = c("children", "adults"))) %>% 
  ggplot(aes(x = param_value, color = condition)) +
  geom_line(stat = "density", size = 1, position = position_dodge(width=0.1)) +
  ggthemes::scale_color_ptol() +
  facet_grid(age_code~param_name, scales = "free") +
  labs(x = "Parameter Estimate", y = NULL, color = "Processing context:") +
  theme_minimal() +
  theme(panel.border = element_rect(fill = NA, color = "grey80"),
        legend.position = "top",
        axis.ticks = element_blank(), 
        axis.text.y = element_blank(),
        text = element_text(size = 10),
        legend.title = element_text(size =9)) 
```


After fitting the EWMA, we took the shifts that were categorized as language-driven and fit a hierarchical Bayesian drift-diffusion model (HDDM). This model quantifies differences in separable parameters of the underlying decision process that lead to different patterns of behavior. The model assumes that people accumulate noisy evidence in favor of one alternative with a response generated when the evidence crosses a pre-defined decision threshold. Here, we focus on two parameters of interest: **boundary separation**, which indexes the amount of evidence gathered before generating a response (higher values suggest more cautious responding) and **drift rate**, which indexes the amount of evidence accumulated per unit time (higher values suggest more efficient processing). 

```{r noise filter}
d_e1_analysis <- d_e1 %>% 
  filter(rt <= 2,
         response_onset_type == "noun",
         shift_start_location == "center") %>% 
  mutate(shift_acc_num = ifelse(shift_accuracy_clean == "correct", 1, 0),
         log_rt = log(rt))
```

```{r noise summarize model output}
ms_acc_e1 <- d_models$acc_noise %>% 
  group_by(noise_condition, age_category) %>% 
  summarise(prop = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 

ms_rt_e1 <- d_models$rt_noise %>% 
  group_by(noise_condition, age_category) %>% 
  summarise(m_rt = mean(rt_ms_scale),
            hdi_lower = quantile(rt_ms_scale, probs = 0.025),
            hdi_upper = quantile(rt_ms_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(age_category = factor(age_category) %>% fct_rev()) 
```

```{r noise create contrasts}
noise_contrast <- d_models$acc_noise %>% 
  select(sample_id:acc_prob_scale, -param_est) %>% 
  spread(noise_condition, acc_prob_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)

noise_contrast_rt <- d_models$rt_noise %>% 
  select(sample_id:rt_ms_scale, -param_est) %>% 
  spread(noise_condition, rt_ms_scale) %>% 
  mutate(noise_contrast = noise - clear) %>% 
  select(sample_id, noise_contrast, age_category)
```

### Behavioral analyses

```{r ms contrast noise rt}
ms_noise_con_rt <- noise_contrast_rt %>%
  mutate(noise_contrast = noise_contrast * 1000) %>% 
  summarise(m_rt = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

**RT.** To make RTs more suitable for modeling on a linear scale, we analyzed responses in log space with the final model specified as: \texttt{$log(RT) \sim noise\_condition + age\_group + (sub\_id + noise\_condition \mid item)$}. Panel A of Figure 2 shows the full RT data distribution, the estimates of condition means, and the full posterior distribution of the estimated difference between the noise and clear conditions. Both children and adults were slower to identify the target in the noise condition (Children $M_{noise}$ = `r ms_rt_e1$m_rt[4]` ms; Adult $M_{noise}$ = `r ms_rt_e1$m_rt[3]` ms), as compared to the clear condition (Children $M_{clear}$ = `r ms_rt_e1$m_rt[2]` ms; Adult $M_{clear}$ = `r ms_rt_e1$m_rt[1]` ms). RTs in the noise condition were `r ms_noise_con_rt$m_rt[1]` ms slower on average, with a 95% HDI from `r ms_noise_con_rt$hdi_lower[1]` ms to `r ms_noise_con_rt$hdi_upper[1]` ms that did not include the null value of zero condition difference.

```{r ms contrast noise acc}
ms_noise_con_acc <- noise_contrast %>%
  summarise(m_acc = mean(noise_contrast),
            hdi_lower = quantile(noise_contrast, probs = 0.025),
            hdi_upper = quantile(noise_contrast, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

## hypothesis test
prob_diff0 <- noise_contrast %>% 
  summarise(prob = mean(noise_contrast >= 0)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

**Accuracy.** Next, we modeled adults' and children's first shift accuracy using a mixed-effects logistic regression with the same specifications (see Panel B of Fig 2). Both groups were more accurate than a model of random responding (null value of $0.5$ falling well outside the lower bound of the 95% HDI for all group means). Adults were more accurate ($M_{adults} =$ `r ms_acc_e1$prop[1] * 100`%) than children ($M_{children} =$ `r ms_acc_e1$prop[2] * 100`%). The key result is that both groups showed evidence of higher accuracy in the noise condition (Children $M_{noise}$ = `r ms_acc_e1$prop[4]* 100`%; Adult $M_{noise}$ = `r ms_acc_e1$prop[3]* 100`%) as compared to the clear condition (Children $M_{clear}$ = `r ms_acc_e1$prop[2]* 100`%; Adult $M_{clear}$ = `r ms_acc_e1$prop[1]* 100`%). Accuracy in the noise condition was `r ms_noise_con_acc$m_acc[1] * 100`%  higher on average, with a 95% HDI from `r ms_noise_con_acc$hdi_lower[1]* 100`% to `r ms_noise_con_acc$hdi_upper[1] * 100`%. Note that the null value of zero difference falls at the very edge of the 95% HDI such that `r prob_diff0$prob[1] * 100`% of the credible values fall below the null, providing evidence for higher accuracy in the noise condition.

### Model-based analyses

```{r noise ewma group means summary}
# summarise group means for cutoffs
ms_cuts_noise <- d_models$ewma_cuts_noise %>% 
  group_by(age_category, noise_condition) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate(model = "EWMA", parameter = "cut point") %>% 
  select(model, parameter, everything())

# summarise group means for prop guessing parameter
ms_guess_noise <- d_models$ewma_guess_noise %>% 
  group_by(age_category, noise_condition) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate(model = "EWMA", parameter = "prob. guessing") %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  select(model, parameter, everything())
```

```{r ewma cond difference summary}
## cut point model noise
ms_cond_diff_cuts <- d_models$ewma_cuts_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  rename(`age group` = age_beta, noise = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing <- d_models$ewma_guess_noise %>% 
  select(sample_id, noise_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`adults-children` = age_beta, 
         `noise-clear` = noise_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E1") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## cut point model gaze
ms_cond_diff_cuts_gaze <- d_models$ewma_cuts_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  rename(`age group` = age_beta, gaze = straightahead_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>%
  mutate(Parameter = "Cut point",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)

## prop guessing model noise
ms_cond_diff_guessing_gaze <- d_models$ewma_guess_gaze %>% 
  select(sample_id, straightahead_beta, age_beta) %>%
  mutate(age_beta = age_beta * -1) %>% 
  rename(`gaze-straight_ahead` = straightahead_beta, 
         `adults-children` = age_beta) %>% 
  unique() %>% 
  gather(key = Contrast, value = param_est, -sample_id) %>% 
  group_by(Contrast) %>% 
  summarise(MAP = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(Parameter = "Guessing",
         `Estimate (95% HDI)` = paste0(MAP, " [", hdi_lower, ", ", hdi_upper, "]"),
         `Exp.` = "E2") %>%
  select(`Exp.`, Parameter, Contrast, `Estimate (95% HDI)`)
```

**EWMA.** Figure 3 shows the proportion of shifts that the model classified as random vs. language-driven for each age group and processing contxt. Critically, processing speech in noise caused both adults and children to produce a higher proportion of language-driven shifts with the 95% HDI excluding the null value (see Table 1). This pattern suggests that the noise condition led participants to increase visual fixations to the language source, leading them to generate fewer exploratory, random shifts before accumulating sufficient information to respond accurately.

**HDDM.** Figure 4 shows the full posterior distributions for the HDDM output. Children had lower drift rates and boundary separation estimates as compared to adults, suggesting that children were less efficient and less cautious in their responding (see also Table 2). Intrestingly, the noise manipulation only affected the boundary separation parameter, with higher estimates in the noise condition for both age groups. This result suggests that participants' in the noise condition prioritized information accumulation over speed when generating an eye movement in response to the incoming language. This increased decision threshold led to higher accuracy. Moreover, the high overlap in estimates of drift rate suggests that participants were able to integrate the visual and auditory signals such that they could achieve a level of processing efficiency comparable to the clear processing context.

Together, the behavioral and EWMA/HDDM results provide converging support for the predictions of our information-seeking account. In summary, processing speech in noise caused listeners to seek additional visual information to support language comprehension. Moreover, we observed a strikingly similar pattern of behavior in children and adults, with both groups producing more language-driven shifts and prioritizing accuracy over speed in the more challenging, noisy context. These data have interesting parallels to recent work from @mcmurray2017waiting, showing that adults with Cochlear Implants, who consistently process degraded auditory input, will delay lexical access and wait until substantial information has accumulated. This process is in contrast to the "immediate competition" model of word recognition where listeners activate candidate meanings from word onset.

```{r ewma results table, results="asis", include = T}
ewma_table <- bind_rows(ms_cond_diff_cuts, ms_cond_diff_guessing) %>% 
  select(-Exp.)

ewma_table <- xtable::xtable(ewma_table, 
                             caption = "EWMA results for E1 and E2. The guessing parameter refers to the proportion of gaze shifts classified as random vs. language-driven with higher values indicating more random responding. Estimate refers to the difference between condition or age group.",
                             align = "lllr")

xtable::print.xtable(ewma_table, type="latex", comment = F, table.placement = "t",
      floating.environment = "table", 
      include.rownames=FALSE)
```

```{r hddm table}
hddm_table <- d_hddm %>% 
  filter(experiment %in% c("noise_gaze", "noise"),
         condition %in% c("straight_ahead_noise", "straight_ahead_clear",
                          "clear", "noise")) %>%
  mutate(Condition = ifelse(str_detect(condition, "noise"), "noise", "clear"),
         `Age Group` = ifelse(str_detect(age_code, "kid"), "children", "adults")) %>% 
  group_by(param_name, Condition, `Age Group`) %>% 
  summarise(MAP = mean(param_value),
            HDI_lower = quantile(param_value, probs = 0.025),
            HDI_upper = quantile(param_value, probs = 0.975)) %>% 
  rename(Parameter = param_name) %>% 
  mutate_at(vars(MAP, HDI_lower, HDI_upper), round, digits = 2) %>% 
  mutate_all(as.character) %>% 
  mutate(`Estimate [95% HDI]` = paste0(MAP, " [", HDI_lower, ", ", HDI_upper, "]")) %>% 
  select(-MAP, -HDI_lower, -HDI_upper)
```

# General Discussion

Language comprehension in grounded contexts involves integrating the visual and linguistic signals. But the value of gathering visual information can vary depending on features of the processing context. Here, we presnted a strong test of an information-seeking account of eye movements during language processing -- an ccount that we first proposed in @macdonald2017info to explain population-level differences in the dynamics of gaze between children learning ASL and children learning spoken English. Here, we showed that children and adults adapt to processing speech in noise by producing slower but more accurate gaze shifts away from a speaker. Both groups also showed evidence of prioritizing information accumulation over speed (HDDM) while producing more language driven shifts (EWMA). It is interesting that listeners were able to achieve higher accuracy in the more challenging, noisy context. Together, the behavioral and modeling results suggest that when the linguistic signal is degraded, listeners adapt their eye movements to seek language-relevant information in the visual world.

```{r print hddm table, include = T, results = "asis"}
hddm_table <- xtable::xtable(hddm_table, 
                             caption = "HDDM parameter estimates for each age group and noise condition. The drift rate parameter indexes processing efficiency and the boundary separation parameter indexes participants' information accumulation threshold.",
                             align = "llllr")

hlines <- c(-1, 0, 4, nrow(hddm_table))

xtable::print.xtable(hddm_table, type="latex", comment = F, table.placement = "t",
      hline.after = hlines,
      floating.environment = "table", 
      include.rownames=FALSE)
```

These results bring together ideas from several research programs. First, work on language-mediated visual attention shows that adults and children rapidly shift gaze upon hearing the name of an object in the visual scene [@allopenna1998tracking; @tanenhaus1995integration]. The speed and consistency of this response has led to debates about whether language-mediated gaze shifts are automatic as opposed to under the control of the listener. While we do claim that listeners in our task have explicit access to the underlying decision process, our findings show that the dynamics of gaze during lexical access adapt to the information features of the context. This finding parallels recent work by @mcmurray2017waiting, showing that adults with Cochlear Implants, who consistently process degraded auditory input, will delay the process of lexical access, waiting to begin until substantial information has accumulated.  

Second, empirical work on vision during natural tasks shows that people overwhelmingly prefer to look at *goal-relevant* locations -- e.g., an upcoming obstacle while walking [@hayhoe2005eye]. These accounts inspired our prediction that gaze dynamics during language comprehension should adapt to the value of different fixation behaviors with respect to the listener's goal of rapid language processing. And third, work on effortful listening shows that listeners generate compensatory responses (e.g., increases in attention and working memory) within "challenging" comprehension contexts such as processing noisy or accented speech [@van2014listening]. These accounts predict that our young listeners might compensate for the reduced quality of the auditory signal by allocating gathering additional visual information.

This work has several important limitations that we hope will pave the way for future work. Here, we chose to focus on a single decision about visual fixation to provide a window onto the underlying dynamics of decision-making across different processing contexts. However, the decision to shift away from a language is just one of the many decisions that listeners make while processing language in real-time. Moreover, our micro-level analysis does not consider the rich gaze patterns that occur before this decision. In our future work, we aim to quantify changes in the dynamics of gaze across the full sentence processing context. Finally, we used a simple visual world, with only three places to look, and very simple linguistic stimuli, especially for the adults. Thus it remains an open question how these results might scale up to more complex language information and visual environments.

We designed this experiment as a strong test our information-maximization proposal in the domain of familiar language comprehension. However, we think that the account is more general. And we are interested in applying this framework -- the in-depth analysis of decisions about visual fixation -- to the language acquisition context. Consider that early in language learning children are acquiring novel word-object links while also learning about visual object categories. Both of these tasks produce goals that should in turn modulate children's decisions about where to allocate visual attention, e.g., seeking nonlinguistic cues to reference such as eye gaze and pointing become critical when you are unfamiliar with the information in the linguistic signal. More generally, we think that these results contribute to a recent theoretical emphasis on including goal-based accounts of eye movements during language comprehension [@salverda2011goal]. We hope that our approach presents a way forward for explaining fixation behaviors across a wider variety of populations, processing contexts, and during different stages of language learning.

# Acknowledgements

We are grateful to the families who participated in this research. Thanks to Tami Alade and Hannah Slater for help with data collection. This work was supported by an NSF GRFP to KM.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
